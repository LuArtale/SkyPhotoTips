{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ejercicio 1 SOA.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMAaIT5chnkmX+RtWvHUz3l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LuArtale/SkyPhotoTips/blob/master/HPC/Artale_Luciano_ejercicio_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgCUzYyKMF4S"
      },
      "source": [
        "# **Ejercicio 1: Invertir Vector**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdjakVuWXh7U"
      },
      "source": [
        "## Introduccion:\n",
        "\n",
        "En este cuaderno se demuestran las diferencias de ejecucion para una funcion que tiene como objetivo la inversion de un vector utilizando primero la ejecucion en CPU con python y luego la ejecucion en GPGPU con python y CUDA."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4yWZ-bLbMPM"
      },
      "source": [
        "## Armado del ambiente:\n",
        "\n",
        "Para poder ejecutar el ejercicio tanto en CPU como para GPGPU no se debe ejecutar ningun comando previo, pero si se debe indicar como parametro la cantidad de elementos del vector a invertir (los mismos son numeros decimales generados de forma aleatoria) y el valor maximo y minimo que pueden tomar esos elementos.\n",
        "\n",
        "Ademas de esto, para CPU se debe seleccionar el entorno de ejecucion \"None\" y para GPGPU se debe seleccionar el entorno \"GPU\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zY7bXCT6Nu7a"
      },
      "source": [
        "## **Ejecucion para CPU:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qEjDLGTIr83",
        "outputId": "d1268448-65b4-455a-f47c-e6bf8ca51ad2"
      },
      "source": [
        "#@title Par치metros de ejecuci칩n CPU { vertical-output: true }\n",
        "# Parametros\n",
        "cantidad_elementos = 1000#@param {type: \"number\"}\n",
        "limite_inferior = -5#@param {type: \"number\"}\n",
        "limite_superior = 5#@param {type: \"number\"}\n",
        "\n",
        "# --------------------------------------------\n",
        "# Importacion de bibliotecas\n",
        "from datetime import datetime\n",
        "tiempo_total = datetime.now()\n",
        "import numpy\n",
        "\n",
        "# --------------------------------------------\n",
        "# Definici칩n de funci칩n que transforma el tiempo en  milisegundos \n",
        "tiempo_en_ms = lambda dt:(dt.days * 24 * 60 * 60 + dt.seconds) * 1000 + dt.microseconds / 1000.0\n",
        "\n",
        "# --------------------------------------------\n",
        "# Defino la memoria de los vectores en cpu\n",
        "#try\n",
        "vector_original_cpu = numpy.random.uniform( limite_inferior, limite_superior, cantidad_elementos )\n",
        "vector_original_cpu = vector_original_cpu.astype( numpy.float32() )\n",
        "\n",
        "# El vector resultado (invertido) se define vacio\n",
        "vector_invertido_cpu = numpy.empty_like( vector_original_cpu )\n",
        "\n",
        "# --------------------------------------------\n",
        "# Realizo la inversion de vectores\n",
        "\n",
        "tiempo_bucle_inversion = datetime.now()\n",
        "\n",
        "# Solo se recorre hasta la mitad del vector ya que se procesan 2 posiciones a la vez\n",
        "cantidad_a_recorrer = cantidad_elementos // 2\n",
        "\n",
        "# Bucle de inversion\n",
        "for idx in range( 0, cantidad_a_recorrer ):\n",
        "  vector_invertido_cpu[idx] = vector_original_cpu[cantidad_elementos-idx-1]\n",
        "  vector_invertido_cpu[cantidad_elementos-idx-1] = vector_original_cpu[idx]\n",
        "\n",
        "tiempo_bucle_inversion = datetime.now() - tiempo_bucle_inversion\n",
        "\n",
        "# --------------------------------------------\n",
        "# Redondeo los elementos de los vectores a dos decimales\n",
        "vector_original_cpu_rounded = [round(x,2) for x in vector_original_cpu]\n",
        "vector_invertido_cpu_rounded = [round(x,2) for x in vector_invertido_cpu]\n",
        "\n",
        "# Informo los resultados\n",
        "print(\"Vectores: \")\n",
        "print( \"* Vector Original: \" )\n",
        "print( vector_original_cpu_rounded )\n",
        "print( \"------------------------------------\")\n",
        "print( \"* Vector Invertido: \" )\n",
        "print( vector_invertido_cpu_rounded )\n",
        "print( \"------------------------------------\")\n",
        "\n",
        "tiempo_total = datetime.now() - tiempo_total\n",
        "\n",
        "print(\"Tiempos de ejecucion en CPU: \")\n",
        "print(\"* Tiempo Total: \", tiempo_en_ms( tiempo_total ), \"[ms]\" )\n",
        "print(\"* Tiempo inversion del vector: \", tiempo_en_ms( tiempo_bucle_inversion ), \"[ms]\" )"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vectores: \n",
            "* Vector Original: \n",
            "[-4.83, -3.88, -3.56, 2.66, -3.94, 2.1, -0.9, 1.14, -4.2, -2.07, -1.96, -0.26, -0.02, 4.18, -0.7, -2.89, -4.17, -4.03, 4.89, 3.05, -1.11, -2.1, -3.13, 2.85, -1.94, -2.92, -3.98, 4.99, -2.77, -1.64, -0.93, 1.65, -4.78, 2.13, 0.18, 2.69, -3.88, -3.74, -4.97, 2.7, -3.52, 4.06, 0.46, 4.1, -2.33, 4.61, 4.48, 2.52, -1.23, -1.31, -1.55, 4.29, -3.27, 1.02, 1.26, 1.29, -4.34, -2.96, 1.37, -0.59, -3.21, -2.71, -3.0, -0.41, -3.9, 0.61, 0.23, 4.27, 1.59, -0.11, 2.05, 3.85, 1.72, -1.13, 1.94, -0.75, 0.32, 3.95, -1.65, 4.86, -2.48, -2.32, 2.26, -2.27, -4.5, 0.57, -2.3, 1.68, -2.1, 4.4, -2.36, -2.31, -2.73, -4.69, -1.18, -4.39, 2.64, -3.16, 3.02, 2.72, 3.28, 3.5, -2.49, 2.24, -2.2, 1.72, 0.46, 0.64, -2.16, 4.11, -2.87, 0.67, 4.91, -0.34, -0.91, 1.9, 4.62, -2.53, -2.92, -2.89, -3.28, -1.92, -4.3, 1.16, 1.8, -0.03, 1.75, -0.78, -2.82, -0.91, -3.23, -1.31, -3.94, -3.28, -2.33, 1.69, -1.97, -0.73, -3.88, -1.42, 1.06, -3.62, 0.08, 3.6, -1.11, 0.27, -2.05, -1.3, 4.75, 1.13, -2.19, 1.93, -3.18, -3.62, 4.98, -1.81, -1.55, -3.49, -1.5, -4.88, 4.21, 4.57, 2.69, -2.27, -3.17, 1.75, 2.76, 4.24, 2.75, 0.03, 4.5, 0.89, -2.86, -0.52, 3.22, 4.28, 3.76, 0.41, -3.64, -0.92, -0.79, 1.09, 2.27, 3.8, 0.98, 3.96, 3.33, -1.1, -4.92, 3.91, -2.73, -2.7, 1.24, -4.61, -4.48, 0.7, -1.22, 4.26, 2.61, 4.28, -4.24, 4.63, -1.28, 0.44, 2.37, 3.84, -2.88, 1.69, -2.87, -2.75, 2.15, 2.11, 2.56, -3.06, 3.34, 1.99, -2.41, 3.29, -3.28, 2.23, 2.35, 0.08, 4.06, 0.71, -0.42, -1.19, -4.89, -4.64, -0.67, -3.35, -2.94, 0.28, -1.11, -3.38, 3.21, -2.81, -4.58, 2.14, 4.74, -3.84, 0.93, 0.62, 4.94, 4.68, 2.78, 3.62, 2.44, -3.43, 1.05, -0.15, -4.21, 4.1, 4.45, 1.25, 3.81, 4.98, 3.71, 1.84, 2.46, 3.44, -2.41, 3.96, 0.6, 4.14, -2.51, 1.76, 0.66, -3.41, 4.49, 1.88, 1.09, 0.19, -1.93, 1.1, -0.32, 1.68, 1.22, -3.07, 4.9, 1.78, -4.62, 1.26, -3.45, -1.75, -2.16, -1.74, 4.16, 1.01, -3.96, 3.24, 3.67, 0.95, -4.66, -1.49, -3.21, -3.79, 1.47, 1.61, -3.52, -3.21, 0.69, 4.21, -1.72, 4.83, 4.38, 2.13, 4.77, 2.68, -0.25, -4.27, 3.54, -0.63, 2.22, -1.41, -0.83, 2.66, 3.99, 1.89, 3.7, 3.73, -2.78, 2.18, 2.93, 0.23, -1.31, 4.16, 1.75, -4.01, 3.44, -3.74, 1.03, 0.31, -3.65, 3.17, 1.55, 4.6, 2.97, -0.46, 4.17, 3.17, 2.8, -0.81, -4.02, 2.99, 2.73, 0.26, -3.21, -1.18, -4.37, -2.76, -1.67, -0.59, -0.25, -1.5, -2.9, 4.72, -2.61, 0.15, -3.1, -3.32, 4.72, -2.51, 2.66, -3.59, 2.16, 0.58, 2.79, -3.57, -4.37, 2.4, -2.74, -1.4, 0.86, 4.2, 4.14, 2.18, -0.03, -4.89, -1.81, -0.42, -4.77, -2.31, 4.57, 2.88, -4.94, -4.33, -0.44, 3.16, 1.34, 0.12, -0.4, 4.73, -0.58, -3.01, 2.64, 1.72, 4.95, 3.89, 0.83, -1.15, 1.58, -1.04, -1.81, 2.29, 1.81, -1.62, 3.38, 1.52, -4.21, -4.67, 3.47, -3.44, 2.46, -4.6, -4.37, -0.69, 4.58, -3.67, -0.64, -4.11, -1.24, -4.69, -3.34, 4.4, 4.14, 2.65, -1.88, 4.51, 0.49, -3.27, -1.35, 2.11, -4.65, 2.28, 3.82, -4.98, 3.1, 4.16, -0.36, -4.37, 0.23, -2.23, 0.72, 2.18, -2.13, 1.74, -3.65, 0.01, 3.01, 3.65, -4.35, 2.33, 2.51, 2.84, 1.81, 4.06, 3.16, 3.56, 0.57, -4.27, -3.15, 2.6, -4.22, -3.77, 0.63, -0.47, -1.28, 2.94, 1.24, 1.2, -0.9, 0.44, 1.8, 3.85, -4.4, -0.23, 1.35, 1.88, 1.81, -3.22, 2.74, -0.66, 2.86, -1.55, 2.45, -3.27, 0.35, -4.12, -3.2, -0.36, -3.17, -3.38, -4.43, 4.75, -4.97, 4.91, -4.71, -3.67, -0.95, -0.49, -1.68, -4.76, -2.6, -2.49, -4.59, 1.38, 2.55, -3.83, -1.69, 1.74, 1.28, -0.5, 2.23, -2.0, -2.99, 2.53, 0.24, 0.84, 1.85, 2.06, 1.86, 2.41, 2.44, -3.3, 4.62, 2.65, -0.37, -0.6, -3.66, 2.04, 2.11, 2.86, 3.57, 2.14, 4.42, 3.85, -3.28, 4.13, -0.29, 0.75, 4.34, -3.93, -4.7, 3.95, -2.62, 1.39, -4.71, -4.82, -1.97, 1.43, -4.82, -1.91, -3.18, 0.78, -3.01, -0.32, 1.46, -1.34, -2.93, -4.98, 0.27, -4.9, 4.12, 3.0, -2.38, 3.67, 0.48, -1.56, -1.17, -2.04, -1.59, -0.3, 0.89, -1.93, -3.95, 3.93, 3.35, -4.83, 4.04, 1.52, 1.11, -4.75, 3.62, -2.44, 2.95, -2.14, 0.65, 4.96, -4.75, -1.91, -1.41, 4.64, -2.53, -0.07, -0.98, 4.09, -4.29, -1.71, -3.36, 4.58, 0.99, 0.84, -1.28, 0.26, 1.88, -4.22, -3.42, 0.95, 2.16, 0.82, 3.56, 0.12, -2.89, -2.38, -2.2, 3.81, 2.98, 0.9, 3.41, -4.92, -4.81, 1.69, 3.47, 2.01, -3.81, -0.28, -1.57, 1.84, 2.23, -0.66, 2.58, 0.6, -2.69, -3.51, 0.96, -0.48, 2.38, 0.14, 1.47, -2.43, 0.74, -4.0, 4.59, -3.83, -4.05, -0.22, -2.32, 2.55, -2.94, 0.9, 4.55, 4.95, 0.12, -3.98, 2.73, -1.16, 4.05, -4.67, 4.45, -2.25, 1.39, 2.76, -0.51, 3.91, -4.13, -2.07, 0.45, -1.04, -4.7, -0.76, 3.02, 2.47, 3.99, 2.96, -4.7, 3.57, -4.28, -0.62, 2.84, -0.02, 1.23, 3.38, 0.63, -3.61, -0.35, 1.62, -1.99, -4.92, 1.62, -4.36, 2.09, -2.65, 4.14, 4.01, 3.18, 2.42, 3.69, -2.79, -0.56, -3.19, -3.77, -4.0, -4.91, -1.99, -4.03, -3.76, -3.11, -3.65, -1.3, -1.13, 2.96, 2.55, -3.35, -0.28, 1.06, -1.02, 2.74, 4.67, -2.39, 4.44, -4.88, -4.74, -0.01, 4.04, 4.34, -2.38, -1.78, -3.04, 1.52, -0.4, 1.07, 3.52, -3.85, -4.21, -2.1, -4.34, 0.13, -2.5, 0.15, 4.46, 2.17, -0.14, -4.18, 2.5, 2.97, -3.31, 4.71, 2.06, -1.38, 4.49, -1.33, 4.36, 3.4, -3.88, 3.51, -0.93, 2.03, -1.86, 4.79, -4.72, -2.43, 0.84, 0.08, 3.72, 4.68, -0.13, 1.26, -3.13, -3.35, 1.93, -2.21, -3.2, -1.1, 0.54, 0.71, -1.63, 2.79, 3.27, 3.98, -1.31, -1.52, -2.58, 4.36, -0.27, -0.75, -2.35, -0.05, 4.1, -3.36, -0.28, 1.97, 2.9, 1.01, -4.26, -3.34, -0.64, -4.8, -2.3, 0.64, -1.13, 3.45, 4.12, -1.3, 0.89, -1.35, -3.46, 2.88, -1.4, 0.82, 4.6, -1.3, 4.88, 2.95, 2.8, -1.65, -2.28, 2.02, 2.11, -0.8, 2.48, -3.11, -0.65, -2.16, 2.12, -3.62, 4.19, 3.9, -3.83, 0.98, 2.64, 2.8, -0.8, -4.43, 0.46, 0.96, 2.57, 0.14, 4.34, 0.47, 0.94, 3.13, 1.67, -1.92, -0.99, -4.09, -3.25, 3.03, 2.42, -1.68, -1.72, -4.15, -1.31, -0.01, -0.07, 4.72, 1.2, 0.59, 4.5, 0.62, 3.22, 2.69, -3.17, -1.26, -2.03, 0.11, 3.36, -0.28, 0.9, -3.06, 4.1, -0.44, -1.1, 4.78, -1.45, -2.97, -1.07, 0.66, 0.71, 3.27, -2.86, -1.78, -0.93, -4.28, 4.41, -3.51, 1.85, -1.52, -3.74, 3.99, -2.61, -2.1, 0.96, -4.93, 4.01, -1.09, 0.42, 1.52, -2.43, -1.68, -3.47, -0.8, 0.24, -1.78, -2.65, 1.75, -1.96, -0.23, -2.92, 4.18, 2.16, 3.84, -4.68, 4.76, -2.72, -0.28, -0.46, -4.57, -1.36, -2.11, 3.45, -0.27, 2.33, -3.41, -4.98, 3.15, -4.56, 3.16, 0.11, 2.59, -4.73, 4.69, -1.88, -3.58, -3.61, -3.22, -0.54, 4.41, 1.2, -4.84, 4.7, 1.21, -2.09, -1.29, -3.46, 2.81, 2.61, 1.09, 2.52, -0.94, -3.13, -0.45, 1.27, -3.27, -0.94, 1.12, -3.1, 4.79, 1.59, 1.77, -0.54, 3.14, 0.27, -0.62, 4.97, -3.07, 0.14, -4.2, -0.98, 1.99, 2.4, -4.66, -2.66, -0.53, 0.29, -4.89, 0.38, 4.76, -3.85, 0.2, -4.27, 0.91, 3.55, -4.72, -3.27, -1.12, 3.97, 0.5, -1.18, 3.14, -1.93, 2.76, 4.71, 1.12, -4.75, -2.22, -1.54, -4.51, -1.91, 0.61, 0.91]\n",
            "------------------------------------\n",
            "* Vector Invertido: \n",
            "[0.91, 0.61, -1.91, -4.51, -1.54, -2.22, -4.75, 1.12, 4.71, 2.76, -1.93, 3.14, -1.18, 0.5, 3.97, -1.12, -3.27, -4.72, 3.55, 0.91, -4.27, 0.2, -3.85, 4.76, 0.38, -4.89, 0.29, -0.53, -2.66, -4.66, 2.4, 1.99, -0.98, -4.2, 0.14, -3.07, 4.97, -0.62, 0.27, 3.14, -0.54, 1.77, 1.59, 4.79, -3.1, 1.12, -0.94, -3.27, 1.27, -0.45, -3.13, -0.94, 2.52, 1.09, 2.61, 2.81, -3.46, -1.29, -2.09, 1.21, 4.7, -4.84, 1.2, 4.41, -0.54, -3.22, -3.61, -3.58, -1.88, 4.69, -4.73, 2.59, 0.11, 3.16, -4.56, 3.15, -4.98, -3.41, 2.33, -0.27, 3.45, -2.11, -1.36, -4.57, -0.46, -0.28, -2.72, 4.76, -4.68, 3.84, 2.16, 4.18, -2.92, -0.23, -1.96, 1.75, -2.65, -1.78, 0.24, -0.8, -3.47, -1.68, -2.43, 1.52, 0.42, -1.09, 4.01, -4.93, 0.96, -2.1, -2.61, 3.99, -3.74, -1.52, 1.85, -3.51, 4.41, -4.28, -0.93, -1.78, -2.86, 3.27, 0.71, 0.66, -1.07, -2.97, -1.45, 4.78, -1.1, -0.44, 4.1, -3.06, 0.9, -0.28, 3.36, 0.11, -2.03, -1.26, -3.17, 2.69, 3.22, 0.62, 4.5, 0.59, 1.2, 4.72, -0.07, -0.01, -1.31, -4.15, -1.72, -1.68, 2.42, 3.03, -3.25, -4.09, -0.99, -1.92, 1.67, 3.13, 0.94, 0.47, 4.34, 0.14, 2.57, 0.96, 0.46, -4.43, -0.8, 2.8, 2.64, 0.98, -3.83, 3.9, 4.19, -3.62, 2.12, -2.16, -0.65, -3.11, 2.48, -0.8, 2.11, 2.02, -2.28, -1.65, 2.8, 2.95, 4.88, -1.3, 4.6, 0.82, -1.4, 2.88, -3.46, -1.35, 0.89, -1.3, 4.12, 3.45, -1.13, 0.64, -2.3, -4.8, -0.64, -3.34, -4.26, 1.01, 2.9, 1.97, -0.28, -3.36, 4.1, -0.05, -2.35, -0.75, -0.27, 4.36, -2.58, -1.52, -1.31, 3.98, 3.27, 2.79, -1.63, 0.71, 0.54, -1.1, -3.2, -2.21, 1.93, -3.35, -3.13, 1.26, -0.13, 4.68, 3.72, 0.08, 0.84, -2.43, -4.72, 4.79, -1.86, 2.03, -0.93, 3.51, -3.88, 3.4, 4.36, -1.33, 4.49, -1.38, 2.06, 4.71, -3.31, 2.97, 2.5, -4.18, -0.14, 2.17, 4.46, 0.15, -2.5, 0.13, -4.34, -2.1, -4.21, -3.85, 3.52, 1.07, -0.4, 1.52, -3.04, -1.78, -2.38, 4.34, 4.04, -0.01, -4.74, -4.88, 4.44, -2.39, 4.67, 2.74, -1.02, 1.06, -0.28, -3.35, 2.55, 2.96, -1.13, -1.3, -3.65, -3.11, -3.76, -4.03, -1.99, -4.91, -4.0, -3.77, -3.19, -0.56, -2.79, 3.69, 2.42, 3.18, 4.01, 4.14, -2.65, 2.09, -4.36, 1.62, -4.92, -1.99, 1.62, -0.35, -3.61, 0.63, 3.38, 1.23, -0.02, 2.84, -0.62, -4.28, 3.57, -4.7, 2.96, 3.99, 2.47, 3.02, -0.76, -4.7, -1.04, 0.45, -2.07, -4.13, 3.91, -0.51, 2.76, 1.39, -2.25, 4.45, -4.67, 4.05, -1.16, 2.73, -3.98, 0.12, 4.95, 4.55, 0.9, -2.94, 2.55, -2.32, -0.22, -4.05, -3.83, 4.59, -4.0, 0.74, -2.43, 1.47, 0.14, 2.38, -0.48, 0.96, -3.51, -2.69, 0.6, 2.58, -0.66, 2.23, 1.84, -1.57, -0.28, -3.81, 2.01, 3.47, 1.69, -4.81, -4.92, 3.41, 0.9, 2.98, 3.81, -2.2, -2.38, -2.89, 0.12, 3.56, 0.82, 2.16, 0.95, -3.42, -4.22, 1.88, 0.26, -1.28, 0.84, 0.99, 4.58, -3.36, -1.71, -4.29, 4.09, -0.98, -0.07, -2.53, 4.64, -1.41, -1.91, -4.75, 4.96, 0.65, -2.14, 2.95, -2.44, 3.62, -4.75, 1.11, 1.52, 4.04, -4.83, 3.35, 3.93, -3.95, -1.93, 0.89, -0.3, -1.59, -2.04, -1.17, -1.56, 0.48, 3.67, -2.38, 3.0, 4.12, -4.9, 0.27, -4.98, -2.93, -1.34, 1.46, -0.32, -3.01, 0.78, -3.18, -1.91, -4.82, 1.43, -1.97, -4.82, -4.71, 1.39, -2.62, 3.95, -4.7, -3.93, 4.34, 0.75, -0.29, 4.13, -3.28, 3.85, 4.42, 2.14, 3.57, 2.86, 2.11, 2.04, -3.66, -0.6, -0.37, 2.65, 4.62, -3.3, 2.44, 2.41, 1.86, 2.06, 1.85, 0.84, 0.24, 2.53, -2.99, -2.0, 2.23, -0.5, 1.28, 1.74, -1.69, -3.83, 2.55, 1.38, -4.59, -2.49, -2.6, -4.76, -1.68, -0.49, -0.95, -3.67, -4.71, 4.91, -4.97, 4.75, -4.43, -3.38, -3.17, -0.36, -3.2, -4.12, 0.35, -3.27, 2.45, -1.55, 2.86, -0.66, 2.74, -3.22, 1.81, 1.88, 1.35, -0.23, -4.4, 3.85, 1.8, 0.44, -0.9, 1.2, 1.24, 2.94, -1.28, -0.47, 0.63, -3.77, -4.22, 2.6, -3.15, -4.27, 0.57, 3.56, 3.16, 4.06, 1.81, 2.84, 2.51, 2.33, -4.35, 3.65, 3.01, 0.01, -3.65, 1.74, -2.13, 2.18, 0.72, -2.23, 0.23, -4.37, -0.36, 4.16, 3.1, -4.98, 3.82, 2.28, -4.65, 2.11, -1.35, -3.27, 0.49, 4.51, -1.88, 2.65, 4.14, 4.4, -3.34, -4.69, -1.24, -4.11, -0.64, -3.67, 4.58, -0.69, -4.37, -4.6, 2.46, -3.44, 3.47, -4.67, -4.21, 1.52, 3.38, -1.62, 1.81, 2.29, -1.81, -1.04, 1.58, -1.15, 0.83, 3.89, 4.95, 1.72, 2.64, -3.01, -0.58, 4.73, -0.4, 0.12, 1.34, 3.16, -0.44, -4.33, -4.94, 2.88, 4.57, -2.31, -4.77, -0.42, -1.81, -4.89, -0.03, 2.18, 4.14, 4.2, 0.86, -1.4, -2.74, 2.4, -4.37, -3.57, 2.79, 0.58, 2.16, -3.59, 2.66, -2.51, 4.72, -3.32, -3.1, 0.15, -2.61, 4.72, -2.9, -1.5, -0.25, -0.59, -1.67, -2.76, -4.37, -1.18, -3.21, 0.26, 2.73, 2.99, -4.02, -0.81, 2.8, 3.17, 4.17, -0.46, 2.97, 4.6, 1.55, 3.17, -3.65, 0.31, 1.03, -3.74, 3.44, -4.01, 1.75, 4.16, -1.31, 0.23, 2.93, 2.18, -2.78, 3.73, 3.7, 1.89, 3.99, 2.66, -0.83, -1.41, 2.22, -0.63, 3.54, -4.27, -0.25, 2.68, 4.77, 2.13, 4.38, 4.83, -1.72, 4.21, 0.69, -3.21, -3.52, 1.61, 1.47, -3.79, -3.21, -1.49, -4.66, 0.95, 3.67, 3.24, -3.96, 1.01, 4.16, -1.74, -2.16, -1.75, -3.45, 1.26, -4.62, 1.78, 4.9, -3.07, 1.22, 1.68, -0.32, 1.1, -1.93, 0.19, 1.09, 1.88, 4.49, -3.41, 0.66, 1.76, -2.51, 4.14, 0.6, 3.96, -2.41, 3.44, 2.46, 1.84, 3.71, 4.98, 3.81, 1.25, 4.45, 4.1, -4.21, -0.15, 1.05, -3.43, 2.44, 3.62, 2.78, 4.68, 4.94, 0.62, 0.93, -3.84, 4.74, 2.14, -4.58, -2.81, 3.21, -3.38, -1.11, 0.28, -2.94, -3.35, -0.67, -4.64, -4.89, -1.19, -0.42, 0.71, 4.06, 0.08, 2.35, 2.23, -3.28, 3.29, -2.41, 1.99, 3.34, -3.06, 2.56, 2.11, 2.15, -2.75, -2.87, 1.69, -2.88, 3.84, 2.37, 0.44, -1.28, 4.63, -4.24, 4.28, 2.61, 4.26, -1.22, 0.7, -4.48, -4.61, 1.24, -2.7, -2.73, 3.91, -4.92, -1.1, 3.33, 3.96, 0.98, 3.8, 2.27, 1.09, -0.79, -0.92, -3.64, 0.41, 3.76, 4.28, 3.22, -0.52, -2.86, 0.89, 4.5, 0.03, 2.75, 4.24, 2.76, 1.75, -3.17, -2.27, 2.69, 4.57, 4.21, -4.88, -1.5, -3.49, -1.55, -1.81, 4.98, -3.62, -3.18, 1.93, -2.19, 1.13, 4.75, -1.3, -2.05, 0.27, -1.11, 3.6, 0.08, -3.62, 1.06, -1.42, -3.88, -0.73, -1.97, 1.69, -2.33, -3.28, -3.94, -1.31, -3.23, -0.91, -2.82, -0.78, 1.75, -0.03, 1.8, 1.16, -4.3, -1.92, -3.28, -2.89, -2.92, -2.53, 4.62, 1.9, -0.91, -0.34, 4.91, 0.67, -2.87, 4.11, -2.16, 0.64, 0.46, 1.72, -2.2, 2.24, -2.49, 3.5, 3.28, 2.72, 3.02, -3.16, 2.64, -4.39, -1.18, -4.69, -2.73, -2.31, -2.36, 4.4, -2.1, 1.68, -2.3, 0.57, -4.5, -2.27, 2.26, -2.32, -2.48, 4.86, -1.65, 3.95, 0.32, -0.75, 1.94, -1.13, 1.72, 3.85, 2.05, -0.11, 1.59, 4.27, 0.23, 0.61, -3.9, -0.41, -3.0, -2.71, -3.21, -0.59, 1.37, -2.96, -4.34, 1.29, 1.26, 1.02, -3.27, 4.29, -1.55, -1.31, -1.23, 2.52, 4.48, 4.61, -2.33, 4.1, 0.46, 4.06, -3.52, 2.7, -4.97, -3.74, -3.88, 2.69, 0.18, 2.13, -4.78, 1.65, -0.93, -1.64, -2.77, 4.99, -3.98, -2.92, -1.94, 2.85, -3.13, -2.1, -1.11, 3.05, 4.89, -4.03, -4.17, -2.89, -0.7, 4.18, -0.02, -0.26, -1.96, -2.07, -4.2, 1.14, -0.9, 2.1, -3.94, 2.66, -3.56, -3.88, -4.83]\n",
            "------------------------------------\n",
            "Tiempos de ejecucion en CPU: \n",
            "* Tiempo Total:  14.348 [ms]\n",
            "* Tiempo inversion del vector:  0.374 [ms]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Vy5psYHwyD1"
      },
      "source": [
        "## Tabla de pasos:\n",
        "\n",
        "Procesador | Funcion | Detalle\n",
        "---------- | ------- | --------\n",
        "CPU | @param | Lectura de tama침o y limites de los vectores\n",
        "CPU | import | Importacion de las bibliotecas a utilizar\n",
        "CPU | import | Importacion de las bibliotecas a utilizar\n",
        "CPU | import | Importacion de las bibliotecas a utilizar\n",
        "CPU | import | Importacion de las bibliotecas a utilizar\n",
        "CPU | import | Importacion de las bibliotecas a utilizar\n",
        "CPU | import | Importacion de las bibliotecas a utilizar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4t_AJ8TN1RV"
      },
      "source": [
        "## **Ejecucion para GPGPU:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrGnoNUKN5aO"
      },
      "source": [
        "### Instalacion CUDA para Python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JS_dv7gLOAIi",
        "outputId": "4582c00e-f753-4a68-d81a-43130c9d0201"
      },
      "source": [
        "!pip install pycuda"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pycuda\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/61/47d3235a4c13eec5a5f03594ddb268f4858734e02980afbcd806e6242fa5/pycuda-2020.1.tar.gz (1.6MB)\n",
            "\u001b[K     |郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗| 1.6MB 8.9MB/s \n",
            "\u001b[?25hCollecting pytools>=2011.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b7/30/c9362a282ef89106768cba9d884f4b2e4f5dc6881d0c19b478d2a710b82b/pytools-2020.4.3.tar.gz (62kB)\n",
            "\u001b[K     |郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗| 71kB 10.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: decorator>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from pycuda) (4.4.2)\n",
            "Collecting appdirs>=1.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/00/2344469e2084fb287c2e0b57b72910309874c3245463acd6cf5e3db69324/appdirs-1.4.4-py2.py3-none-any.whl\n",
            "Collecting mako\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/37/0e706200d22172eb8fa17d68a7ae22dec7631a0a92266634fb518a88a5b2/Mako-1.1.3-py2.py3-none-any.whl (75kB)\n",
            "\u001b[K     |郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗| 81kB 10.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.8.0 in /usr/local/lib/python3.6/dist-packages (from pytools>=2011.2->pycuda) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from pytools>=2011.2->pycuda) (1.18.5)\n",
            "Requirement already satisfied: dataclasses>=0.7 in /usr/local/lib/python3.6/dist-packages (from pytools>=2011.2->pycuda) (0.7)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from mako->pycuda) (1.1.1)\n",
            "Building wheels for collected packages: pycuda, pytools\n",
            "  Building wheel for pycuda (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycuda: filename=pycuda-2020.1-cp36-cp36m-linux_x86_64.whl size=621008 sha256=480092353675f58b99cd797a4394c0aea70bfcb4e6b35ce6d0692e3b44b9e922\n",
            "  Stored in directory: /root/.cache/pip/wheels/8f/78/d1/5bb826f81d9d490297a348d818ff3ee6dd6f2075b06dde6ea0\n",
            "  Building wheel for pytools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytools: filename=pytools-2020.4.3-py2.py3-none-any.whl size=61374 sha256=b53b30493371e7e1c36a1ec199e975663a7ecd1acef254cc50b25d35c2c7498e\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/c7/81/a22edb90b0b09a880468b2253bb1df8e9f503337ee15432c64\n",
            "Successfully built pycuda pytools\n",
            "Installing collected packages: appdirs, pytools, mako, pycuda\n",
            "Successfully installed appdirs-1.4.4 mako-1.1.3 pycuda-2020.1 pytools-2020.4.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJEYMAcTOD4p"
      },
      "source": [
        "### Ejecucion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLH_WlYKOIok",
        "outputId": "4e6e2442-cb47-48e1-e0bf-527057d27937"
      },
      "source": [
        "#@title Par치metros de ejecuci칩n GPGPU { vertical-output: true }\n",
        "# Parametros\n",
        "cantidad_elementos = 1000#@param {type: \"number\"}\n",
        "limite_inferior = -5#@param {type: \"number\"}\n",
        "limite_superior = 5#@param {type: \"number\"}\n",
        "\n",
        "# --------------------------------------------\n",
        "# Importacion de bibliotecas\n",
        "from datetime import datetime\n",
        "tiempo_total = datetime.now()\n",
        "\n",
        "import pycuda.driver as cuda\n",
        "import pycuda.autoinit\n",
        "from pycuda.compiler import SourceModule\n",
        "\n",
        "import numpy\n",
        "\n",
        "# --------------------------------------------\n",
        "# Definici칩n de funci칩n que transforma el tiempo en  milisegundos \n",
        "tiempo_en_ms = lambda dt:(dt.days * 24 * 60 * 60 + dt.seconds) * 1000 + dt.microseconds / 1000.0\n",
        "\n",
        "\n",
        "# CPU - Defino la memoria de los vectores en cpu.\n",
        "x_cpu = numpy.random.randn( cantidad_N )\n",
        "#x_cpu = numpy.array([1.00, 2.00, 3.00, 4.00, 5.00, 6.00, 7.00, 8.00, 9.00, 10.00])\n",
        "x_cpu = x_cpu.astype( numpy.float32() )\n",
        "\n",
        "y_cpu = numpy.random.randn( cantidad_N )\n",
        "#y_cpu = numpy.array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00])\n",
        "y_cpu = y_cpu.astype( numpy.float32() )\n",
        "\n",
        "#tiempo_ini_cpu = datetime.now()\n",
        "\n",
        "r_cpu = numpy.empty_like( x_cpu )\n",
        "\n",
        "# CPU - reservo la memoria GPU.\n",
        "x_gpu = cuda.mem_alloc( x_cpu.nbytes )\n",
        "y_gpu = cuda.mem_alloc( y_cpu.nbytes )\n",
        "\n",
        "# GPU - Copio la memoria al GPU.\n",
        "cuda.memcpy_htod( x_gpu, x_cpu )\n",
        "cuda.memcpy_htod( y_gpu, y_cpu )\n",
        "\n",
        "# CPU - Defino la funci칩n kernel que ejecutar치 en GPU.\n",
        "module = SourceModule(\"\"\"\n",
        "__global__ void kernel_invert( int n, float *X, float *Y )\n",
        "{\n",
        "  int idx = threadIdx.x + blockIdx.x*blockDim.x;\n",
        "  //d_a[idx] = threadIdx.x + 1000*blockIdx.x\n",
        "\n",
        "  if( idx < (n/2) )\n",
        "  {\n",
        "    //Y[idx]  = alfa*X[idx] + Y[idx];\n",
        "    Y[idx] = X[n-idx-1];\n",
        "    Y[n-idx-1] = X[idx];\n",
        "  }\n",
        "}\n",
        "\"\"\") \n",
        "# CPU - Genero la funci칩n kernel.\n",
        "kernel = module.get_function(\"kernel_invert\")\n",
        "\n",
        "tiempo_gpu = datetime.now()\n",
        "\n",
        "# GPU - Ejecuta el kernel.\n",
        "# TODO: Falta consultar limites del GPU, para armar las dimensiones correctamente.\n",
        "dim_hilo = 256\n",
        "dim_bloque = numpy.int( (cantidad_N+dim_hilo-1) / dim_hilo )\n",
        "print( \"Thread x: \", dim_hilo, \", Bloque x:\", dim_bloque )\n",
        "\n",
        "#TODO: Ojo, con los tipos de las variables en el kernel.\n",
        "kernel( numpy.int32(cantidad_N), x_gpu, y_gpu, block=( dim_hilo, 1, 1 ),grid=(dim_bloque, 1,1) )\n",
        "\n",
        "tiempo_gpu = datetime.now() - tiempo_gpu\n",
        "\n",
        "# GPU - Copio el resultado desde la memoria GPU.\n",
        "cuda.memcpy_dtoh( r_cpu, y_gpu )\n",
        "\n",
        "#Redondeo los vectores para mostrarlos\n",
        "x_cpu_rounded = [round(x,2) for x in x_cpu]\n",
        "y_cpu_rounded = [round(x,2) for x in y_cpu]\n",
        "r_cpu_rounded = [round(x,2) for x in r_cpu]\n",
        "\n",
        "# CPU - Informo el resutlado.\n",
        "print( \"------------------------------------\")\n",
        "print( \"X: \" )\n",
        "print( x_cpu_rounded )\n",
        "print( \"------------------------------------\")\n",
        "print( \"Y: \" )\n",
        "print( y_cpu_rounded )\n",
        "print( \"------------------------------------\")\n",
        "print( \"R: \" )\n",
        "print( r_cpu_rounded )\n",
        "\n",
        "\n",
        "tiempo_total = datetime.now() - tiempo_total\n",
        "\n",
        "print( \"Cantidad de elementos: \", cantidad_N )\n",
        "print( \"Thread x: \", dim_hilo, \", Bloque x:\", dim_bloque )\n",
        "print(\"Tiempo CPU: \", tiempo_en_ms( tiempo_total ), \"[ms]\" )\n",
        "print(\"Tiempo GPU: \", tiempo_en_ms( tiempo_gpu   ), \"[ms]\" )"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thread x:  256 , Bloque x: 1\n",
            "------------------------------------\n",
            "X: \n",
            "[0.34, -0.21, 1.46, -0.87, 0.87, 1.51, -0.22, 1.15, 1.51, 1.11, -2.22, -0.42, 1.11, -0.13, -0.24, 0.41, -0.83, 0.7, 0.86, 1.93, 1.23, 2.13, -1.81, 0.18, 0.16, -0.06, -0.35, 0.08, 1.2, -0.68, 1.9, 1.05, -0.83, -0.47, 1.54, 1.39, -0.75, -0.99, 0.2, -2.29, -0.9, -0.91, 0.27, 1.65, -1.69, -1.99, -0.37, -0.35, -0.54, -0.82, -0.6, 0.57, 0.66, 0.94, 1.31, 0.58, -0.23, -1.09, -0.01, -0.05, -0.73, 1.33, -0.21, 0.14, 0.44, 0.37, -1.14, 0.11, -2.15, 0.89, 1.45, -0.09, -0.81, -0.36, 0.98, -1.98, 1.09, 2.37, -0.16, -0.17, -2.13, 0.49, 0.31, 0.23, -0.28, -0.8, 1.58, 0.47, 0.18, 0.67, 0.34, 0.19, -1.64, -0.28, 0.26, 0.97, -0.88, 1.08, 0.37, 0.56]\n",
            "------------------------------------\n",
            "Y: \n",
            "[0.55, 0.32, 0.45, -0.08, -0.63, 1.08, 0.16, 0.03, -1.1, -0.06, -0.12, -0.75, -0.17, -1.24, 0.1, -1.59, -0.31, 0.87, 1.17, 0.97, 1.67, -0.94, 2.08, 1.68, 1.14, 0.2, -0.3, -0.06, -0.33, -0.03, -1.5, -0.64, 0.13, 1.65, 0.17, -1.22, 0.15, -0.32, -0.82, 1.17, -0.12, 0.86, -0.24, 1.02, 1.05, 0.17, 1.9, -0.41, -0.53, -0.35, 0.66, -0.04, 0.18, -0.67, 0.46, -1.01, 0.33, -0.61, -0.55, -2.11, -0.12, -0.17, 0.68, 0.37, -0.21, -2.34, -0.33, 0.12, 0.8, -0.33, -1.54, 2.18, -0.34, 2.6, -0.07, 0.15, 1.86, 0.52, 0.27, 0.15, 0.28, -1.07, 2.06, -1.98, 0.62, 0.11, 0.76, -0.1, 0.61, 0.42, -1.21, -1.33, 0.6, 0.71, -1.71, 0.97, 0.18, 0.3, 0.4, 0.85]\n",
            "------------------------------------\n",
            "R: \n",
            "[0.56, 0.37, 1.08, -0.88, 0.97, 0.26, -0.28, -1.64, 0.19, 0.34, 0.67, 0.18, 0.47, 1.58, -0.8, -0.28, 0.23, 0.31, 0.49, -2.13, -0.17, -0.16, 2.37, 1.09, -1.98, 0.98, -0.36, -0.81, -0.09, 1.45, 0.89, -2.15, 0.11, -1.14, 0.37, 0.44, 0.14, -0.21, 1.33, -0.73, -0.05, -0.01, -1.09, -0.23, 0.58, 1.31, 0.94, 0.66, 0.57, -0.6, -0.82, -0.54, -0.35, -0.37, -1.99, -1.69, 1.65, 0.27, -0.91, -0.9, -2.29, 0.2, -0.99, -0.75, 1.39, 1.54, -0.47, -0.83, 1.05, 1.9, -0.68, 1.2, 0.08, -0.35, -0.06, 0.16, 0.18, -1.81, 2.13, 1.23, 1.93, 0.86, 0.7, -0.83, 0.41, -0.24, -0.13, 1.11, -0.42, -2.22, 1.11, 1.51, 1.15, -0.22, 1.51, 0.87, -0.87, 1.46, -0.21, 0.34]\n",
            "Cantidad de elementos:  100\n",
            "Thread x:  256 , Bloque x: 1\n",
            "Tiempo CPU:  258.939 [ms]\n",
            "Tiempo GPU:  0.545 [ms]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}