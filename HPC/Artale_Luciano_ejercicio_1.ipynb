{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ejercicio 1 SOA.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMAaIT5chnkmX+RtWvHUz3l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LuArtale/SkyPhotoTips/blob/master/HPC/Artale_Luciano_ejercicio_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgCUzYyKMF4S"
      },
      "source": [
        "# **Ejercicio 1: Invertir Vector**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdjakVuWXh7U"
      },
      "source": [
        "## Introduccion:\n",
        "\n",
        "En este cuaderno se demuestran las diferencias de ejecucion para una funcion que tiene como objetivo la inversion de un vector utilizando primero la ejecucion en CPU con python y luego la ejecucion en GPGPU con python y CUDA."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4yWZ-bLbMPM"
      },
      "source": [
        "## Armado del ambiente:\n",
        "\n",
        "Para poder ejecutar el ejercicio tanto en CPU como para GPGPU no se debe ejecutar ningun comando previo, pero si se debe indicar como parametro la cantidad de elementos del vector a invertir (los mismos son numeros decimales generados de forma aleatoria) y el valor maximo y minimo que pueden tomar esos elementos.\n",
        "\n",
        "Ademas de esto, para CPU se debe seleccionar el entorno de ejecucion \"None\" y para GPGPU se debe seleccionar el entorno \"GPU\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zY7bXCT6Nu7a"
      },
      "source": [
        "## **Ejecucion para CPU:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qEjDLGTIr83",
        "outputId": "d1268448-65b4-455a-f47c-e6bf8ca51ad2"
      },
      "source": [
        "#@title Parámetros de ejecución CPU { vertical-output: true }\n",
        "# Parametros\n",
        "cantidad_elementos = 1000#@param {type: \"number\"}\n",
        "limite_inferior = -5#@param {type: \"number\"}\n",
        "limite_superior = 5#@param {type: \"number\"}\n",
        "\n",
        "# --------------------------------------------\n",
        "# Importacion de bibliotecas\n",
        "from datetime import datetime\n",
        "tiempo_total = datetime.now()\n",
        "import numpy\n",
        "\n",
        "# --------------------------------------------\n",
        "# Definición de función que transforma el tiempo en  milisegundos \n",
        "tiempo_en_ms = lambda dt:(dt.days * 24 * 60 * 60 + dt.seconds) * 1000 + dt.microseconds / 1000.0\n",
        "\n",
        "# --------------------------------------------\n",
        "# Defino la memoria de los vectores en cpu\n",
        "#try\n",
        "vector_original_cpu = numpy.random.uniform( limite_inferior, limite_superior, cantidad_elementos )\n",
        "vector_original_cpu = vector_original_cpu.astype( numpy.float32() )\n",
        "\n",
        "# El vector resultado (invertido) se define vacio\n",
        "vector_invertido_cpu = numpy.empty_like( vector_original_cpu )\n",
        "\n",
        "# --------------------------------------------\n",
        "# Realizo la inversion de vectores\n",
        "\n",
        "tiempo_bucle_inversion = datetime.now()\n",
        "\n",
        "# Solo se recorre hasta la mitad del vector ya que se procesan 2 posiciones a la vez\n",
        "cantidad_a_recorrer = cantidad_elementos // 2\n",
        "\n",
        "# Bucle de inversion\n",
        "for idx in range( 0, cantidad_a_recorrer ):\n",
        "  vector_invertido_cpu[idx] = vector_original_cpu[cantidad_elementos-idx-1]\n",
        "  vector_invertido_cpu[cantidad_elementos-idx-1] = vector_original_cpu[idx]\n",
        "\n",
        "tiempo_bucle_inversion = datetime.now() - tiempo_bucle_inversion\n",
        "\n",
        "# --------------------------------------------\n",
        "# Redondeo los elementos de los vectores a dos decimales\n",
        "vector_original_cpu_rounded = [round(x,2) for x in vector_original_cpu]\n",
        "vector_invertido_cpu_rounded = [round(x,2) for x in vector_invertido_cpu]\n",
        "\n",
        "# Informo los resultados\n",
        "print(\"Vectores: \")\n",
        "print( \"* Vector Original: \" )\n",
        "print( vector_original_cpu_rounded )\n",
        "print( \"------------------------------------\")\n",
        "print( \"* Vector Invertido: \" )\n",
        "print( vector_invertido_cpu_rounded )\n",
        "print( \"------------------------------------\")\n",
        "\n",
        "tiempo_total = datetime.now() - tiempo_total\n",
        "\n",
        "print(\"Tiempos de ejecucion en CPU: \")\n",
        "print(\"* Tiempo Total: \", tiempo_en_ms( tiempo_total ), \"[ms]\" )\n",
        "print(\"* Tiempo inversion del vector: \", tiempo_en_ms( tiempo_bucle_inversion ), \"[ms]\" )"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vectores: \n",
            "* Vector Original: \n",
            "[-4.83, -3.88, -3.56, 2.66, -3.94, 2.1, -0.9, 1.14, -4.2, -2.07, -1.96, -0.26, -0.02, 4.18, -0.7, -2.89, -4.17, -4.03, 4.89, 3.05, -1.11, -2.1, -3.13, 2.85, -1.94, -2.92, -3.98, 4.99, -2.77, -1.64, -0.93, 1.65, -4.78, 2.13, 0.18, 2.69, -3.88, -3.74, -4.97, 2.7, -3.52, 4.06, 0.46, 4.1, -2.33, 4.61, 4.48, 2.52, -1.23, -1.31, -1.55, 4.29, -3.27, 1.02, 1.26, 1.29, -4.34, -2.96, 1.37, -0.59, -3.21, -2.71, -3.0, -0.41, -3.9, 0.61, 0.23, 4.27, 1.59, -0.11, 2.05, 3.85, 1.72, -1.13, 1.94, -0.75, 0.32, 3.95, -1.65, 4.86, -2.48, -2.32, 2.26, -2.27, -4.5, 0.57, -2.3, 1.68, -2.1, 4.4, -2.36, -2.31, -2.73, -4.69, -1.18, -4.39, 2.64, -3.16, 3.02, 2.72, 3.28, 3.5, -2.49, 2.24, -2.2, 1.72, 0.46, 0.64, -2.16, 4.11, -2.87, 0.67, 4.91, -0.34, -0.91, 1.9, 4.62, -2.53, -2.92, -2.89, -3.28, -1.92, -4.3, 1.16, 1.8, -0.03, 1.75, -0.78, -2.82, -0.91, -3.23, -1.31, -3.94, -3.28, -2.33, 1.69, -1.97, -0.73, -3.88, -1.42, 1.06, -3.62, 0.08, 3.6, -1.11, 0.27, -2.05, -1.3, 4.75, 1.13, -2.19, 1.93, -3.18, -3.62, 4.98, -1.81, -1.55, -3.49, -1.5, -4.88, 4.21, 4.57, 2.69, -2.27, -3.17, 1.75, 2.76, 4.24, 2.75, 0.03, 4.5, 0.89, -2.86, -0.52, 3.22, 4.28, 3.76, 0.41, -3.64, -0.92, -0.79, 1.09, 2.27, 3.8, 0.98, 3.96, 3.33, -1.1, -4.92, 3.91, -2.73, -2.7, 1.24, -4.61, -4.48, 0.7, -1.22, 4.26, 2.61, 4.28, -4.24, 4.63, -1.28, 0.44, 2.37, 3.84, -2.88, 1.69, -2.87, -2.75, 2.15, 2.11, 2.56, -3.06, 3.34, 1.99, -2.41, 3.29, -3.28, 2.23, 2.35, 0.08, 4.06, 0.71, -0.42, -1.19, -4.89, -4.64, -0.67, -3.35, -2.94, 0.28, -1.11, -3.38, 3.21, -2.81, -4.58, 2.14, 4.74, -3.84, 0.93, 0.62, 4.94, 4.68, 2.78, 3.62, 2.44, -3.43, 1.05, -0.15, -4.21, 4.1, 4.45, 1.25, 3.81, 4.98, 3.71, 1.84, 2.46, 3.44, -2.41, 3.96, 0.6, 4.14, -2.51, 1.76, 0.66, -3.41, 4.49, 1.88, 1.09, 0.19, -1.93, 1.1, -0.32, 1.68, 1.22, -3.07, 4.9, 1.78, -4.62, 1.26, -3.45, -1.75, -2.16, -1.74, 4.16, 1.01, -3.96, 3.24, 3.67, 0.95, -4.66, -1.49, -3.21, -3.79, 1.47, 1.61, -3.52, -3.21, 0.69, 4.21, -1.72, 4.83, 4.38, 2.13, 4.77, 2.68, -0.25, -4.27, 3.54, -0.63, 2.22, -1.41, -0.83, 2.66, 3.99, 1.89, 3.7, 3.73, -2.78, 2.18, 2.93, 0.23, -1.31, 4.16, 1.75, -4.01, 3.44, -3.74, 1.03, 0.31, -3.65, 3.17, 1.55, 4.6, 2.97, -0.46, 4.17, 3.17, 2.8, -0.81, -4.02, 2.99, 2.73, 0.26, -3.21, -1.18, -4.37, -2.76, -1.67, -0.59, -0.25, -1.5, -2.9, 4.72, -2.61, 0.15, -3.1, -3.32, 4.72, -2.51, 2.66, -3.59, 2.16, 0.58, 2.79, -3.57, -4.37, 2.4, -2.74, -1.4, 0.86, 4.2, 4.14, 2.18, -0.03, -4.89, -1.81, -0.42, -4.77, -2.31, 4.57, 2.88, -4.94, -4.33, -0.44, 3.16, 1.34, 0.12, -0.4, 4.73, -0.58, -3.01, 2.64, 1.72, 4.95, 3.89, 0.83, -1.15, 1.58, -1.04, -1.81, 2.29, 1.81, -1.62, 3.38, 1.52, -4.21, -4.67, 3.47, -3.44, 2.46, -4.6, -4.37, -0.69, 4.58, -3.67, -0.64, -4.11, -1.24, -4.69, -3.34, 4.4, 4.14, 2.65, -1.88, 4.51, 0.49, -3.27, -1.35, 2.11, -4.65, 2.28, 3.82, -4.98, 3.1, 4.16, -0.36, -4.37, 0.23, -2.23, 0.72, 2.18, -2.13, 1.74, -3.65, 0.01, 3.01, 3.65, -4.35, 2.33, 2.51, 2.84, 1.81, 4.06, 3.16, 3.56, 0.57, -4.27, -3.15, 2.6, -4.22, -3.77, 0.63, -0.47, -1.28, 2.94, 1.24, 1.2, -0.9, 0.44, 1.8, 3.85, -4.4, -0.23, 1.35, 1.88, 1.81, -3.22, 2.74, -0.66, 2.86, -1.55, 2.45, -3.27, 0.35, -4.12, -3.2, -0.36, -3.17, -3.38, -4.43, 4.75, -4.97, 4.91, -4.71, -3.67, -0.95, -0.49, -1.68, -4.76, -2.6, -2.49, -4.59, 1.38, 2.55, -3.83, -1.69, 1.74, 1.28, -0.5, 2.23, -2.0, -2.99, 2.53, 0.24, 0.84, 1.85, 2.06, 1.86, 2.41, 2.44, -3.3, 4.62, 2.65, -0.37, -0.6, -3.66, 2.04, 2.11, 2.86, 3.57, 2.14, 4.42, 3.85, -3.28, 4.13, -0.29, 0.75, 4.34, -3.93, -4.7, 3.95, -2.62, 1.39, -4.71, -4.82, -1.97, 1.43, -4.82, -1.91, -3.18, 0.78, -3.01, -0.32, 1.46, -1.34, -2.93, -4.98, 0.27, -4.9, 4.12, 3.0, -2.38, 3.67, 0.48, -1.56, -1.17, -2.04, -1.59, -0.3, 0.89, -1.93, -3.95, 3.93, 3.35, -4.83, 4.04, 1.52, 1.11, -4.75, 3.62, -2.44, 2.95, -2.14, 0.65, 4.96, -4.75, -1.91, -1.41, 4.64, -2.53, -0.07, -0.98, 4.09, -4.29, -1.71, -3.36, 4.58, 0.99, 0.84, -1.28, 0.26, 1.88, -4.22, -3.42, 0.95, 2.16, 0.82, 3.56, 0.12, -2.89, -2.38, -2.2, 3.81, 2.98, 0.9, 3.41, -4.92, -4.81, 1.69, 3.47, 2.01, -3.81, -0.28, -1.57, 1.84, 2.23, -0.66, 2.58, 0.6, -2.69, -3.51, 0.96, -0.48, 2.38, 0.14, 1.47, -2.43, 0.74, -4.0, 4.59, -3.83, -4.05, -0.22, -2.32, 2.55, -2.94, 0.9, 4.55, 4.95, 0.12, -3.98, 2.73, -1.16, 4.05, -4.67, 4.45, -2.25, 1.39, 2.76, -0.51, 3.91, -4.13, -2.07, 0.45, -1.04, -4.7, -0.76, 3.02, 2.47, 3.99, 2.96, -4.7, 3.57, -4.28, -0.62, 2.84, -0.02, 1.23, 3.38, 0.63, -3.61, -0.35, 1.62, -1.99, -4.92, 1.62, -4.36, 2.09, -2.65, 4.14, 4.01, 3.18, 2.42, 3.69, -2.79, -0.56, -3.19, -3.77, -4.0, -4.91, -1.99, -4.03, -3.76, -3.11, -3.65, -1.3, -1.13, 2.96, 2.55, -3.35, -0.28, 1.06, -1.02, 2.74, 4.67, -2.39, 4.44, -4.88, -4.74, -0.01, 4.04, 4.34, -2.38, -1.78, -3.04, 1.52, -0.4, 1.07, 3.52, -3.85, -4.21, -2.1, -4.34, 0.13, -2.5, 0.15, 4.46, 2.17, -0.14, -4.18, 2.5, 2.97, -3.31, 4.71, 2.06, -1.38, 4.49, -1.33, 4.36, 3.4, -3.88, 3.51, -0.93, 2.03, -1.86, 4.79, -4.72, -2.43, 0.84, 0.08, 3.72, 4.68, -0.13, 1.26, -3.13, -3.35, 1.93, -2.21, -3.2, -1.1, 0.54, 0.71, -1.63, 2.79, 3.27, 3.98, -1.31, -1.52, -2.58, 4.36, -0.27, -0.75, -2.35, -0.05, 4.1, -3.36, -0.28, 1.97, 2.9, 1.01, -4.26, -3.34, -0.64, -4.8, -2.3, 0.64, -1.13, 3.45, 4.12, -1.3, 0.89, -1.35, -3.46, 2.88, -1.4, 0.82, 4.6, -1.3, 4.88, 2.95, 2.8, -1.65, -2.28, 2.02, 2.11, -0.8, 2.48, -3.11, -0.65, -2.16, 2.12, -3.62, 4.19, 3.9, -3.83, 0.98, 2.64, 2.8, -0.8, -4.43, 0.46, 0.96, 2.57, 0.14, 4.34, 0.47, 0.94, 3.13, 1.67, -1.92, -0.99, -4.09, -3.25, 3.03, 2.42, -1.68, -1.72, -4.15, -1.31, -0.01, -0.07, 4.72, 1.2, 0.59, 4.5, 0.62, 3.22, 2.69, -3.17, -1.26, -2.03, 0.11, 3.36, -0.28, 0.9, -3.06, 4.1, -0.44, -1.1, 4.78, -1.45, -2.97, -1.07, 0.66, 0.71, 3.27, -2.86, -1.78, -0.93, -4.28, 4.41, -3.51, 1.85, -1.52, -3.74, 3.99, -2.61, -2.1, 0.96, -4.93, 4.01, -1.09, 0.42, 1.52, -2.43, -1.68, -3.47, -0.8, 0.24, -1.78, -2.65, 1.75, -1.96, -0.23, -2.92, 4.18, 2.16, 3.84, -4.68, 4.76, -2.72, -0.28, -0.46, -4.57, -1.36, -2.11, 3.45, -0.27, 2.33, -3.41, -4.98, 3.15, -4.56, 3.16, 0.11, 2.59, -4.73, 4.69, -1.88, -3.58, -3.61, -3.22, -0.54, 4.41, 1.2, -4.84, 4.7, 1.21, -2.09, -1.29, -3.46, 2.81, 2.61, 1.09, 2.52, -0.94, -3.13, -0.45, 1.27, -3.27, -0.94, 1.12, -3.1, 4.79, 1.59, 1.77, -0.54, 3.14, 0.27, -0.62, 4.97, -3.07, 0.14, -4.2, -0.98, 1.99, 2.4, -4.66, -2.66, -0.53, 0.29, -4.89, 0.38, 4.76, -3.85, 0.2, -4.27, 0.91, 3.55, -4.72, -3.27, -1.12, 3.97, 0.5, -1.18, 3.14, -1.93, 2.76, 4.71, 1.12, -4.75, -2.22, -1.54, -4.51, -1.91, 0.61, 0.91]\n",
            "------------------------------------\n",
            "* Vector Invertido: \n",
            "[0.91, 0.61, -1.91, -4.51, -1.54, -2.22, -4.75, 1.12, 4.71, 2.76, -1.93, 3.14, -1.18, 0.5, 3.97, -1.12, -3.27, -4.72, 3.55, 0.91, -4.27, 0.2, -3.85, 4.76, 0.38, -4.89, 0.29, -0.53, -2.66, -4.66, 2.4, 1.99, -0.98, -4.2, 0.14, -3.07, 4.97, -0.62, 0.27, 3.14, -0.54, 1.77, 1.59, 4.79, -3.1, 1.12, -0.94, -3.27, 1.27, -0.45, -3.13, -0.94, 2.52, 1.09, 2.61, 2.81, -3.46, -1.29, -2.09, 1.21, 4.7, -4.84, 1.2, 4.41, -0.54, -3.22, -3.61, -3.58, -1.88, 4.69, -4.73, 2.59, 0.11, 3.16, -4.56, 3.15, -4.98, -3.41, 2.33, -0.27, 3.45, -2.11, -1.36, -4.57, -0.46, -0.28, -2.72, 4.76, -4.68, 3.84, 2.16, 4.18, -2.92, -0.23, -1.96, 1.75, -2.65, -1.78, 0.24, -0.8, -3.47, -1.68, -2.43, 1.52, 0.42, -1.09, 4.01, -4.93, 0.96, -2.1, -2.61, 3.99, -3.74, -1.52, 1.85, -3.51, 4.41, -4.28, -0.93, -1.78, -2.86, 3.27, 0.71, 0.66, -1.07, -2.97, -1.45, 4.78, -1.1, -0.44, 4.1, -3.06, 0.9, -0.28, 3.36, 0.11, -2.03, -1.26, -3.17, 2.69, 3.22, 0.62, 4.5, 0.59, 1.2, 4.72, -0.07, -0.01, -1.31, -4.15, -1.72, -1.68, 2.42, 3.03, -3.25, -4.09, -0.99, -1.92, 1.67, 3.13, 0.94, 0.47, 4.34, 0.14, 2.57, 0.96, 0.46, -4.43, -0.8, 2.8, 2.64, 0.98, -3.83, 3.9, 4.19, -3.62, 2.12, -2.16, -0.65, -3.11, 2.48, -0.8, 2.11, 2.02, -2.28, -1.65, 2.8, 2.95, 4.88, -1.3, 4.6, 0.82, -1.4, 2.88, -3.46, -1.35, 0.89, -1.3, 4.12, 3.45, -1.13, 0.64, -2.3, -4.8, -0.64, -3.34, -4.26, 1.01, 2.9, 1.97, -0.28, -3.36, 4.1, -0.05, -2.35, -0.75, -0.27, 4.36, -2.58, -1.52, -1.31, 3.98, 3.27, 2.79, -1.63, 0.71, 0.54, -1.1, -3.2, -2.21, 1.93, -3.35, -3.13, 1.26, -0.13, 4.68, 3.72, 0.08, 0.84, -2.43, -4.72, 4.79, -1.86, 2.03, -0.93, 3.51, -3.88, 3.4, 4.36, -1.33, 4.49, -1.38, 2.06, 4.71, -3.31, 2.97, 2.5, -4.18, -0.14, 2.17, 4.46, 0.15, -2.5, 0.13, -4.34, -2.1, -4.21, -3.85, 3.52, 1.07, -0.4, 1.52, -3.04, -1.78, -2.38, 4.34, 4.04, -0.01, -4.74, -4.88, 4.44, -2.39, 4.67, 2.74, -1.02, 1.06, -0.28, -3.35, 2.55, 2.96, -1.13, -1.3, -3.65, -3.11, -3.76, -4.03, -1.99, -4.91, -4.0, -3.77, -3.19, -0.56, -2.79, 3.69, 2.42, 3.18, 4.01, 4.14, -2.65, 2.09, -4.36, 1.62, -4.92, -1.99, 1.62, -0.35, -3.61, 0.63, 3.38, 1.23, -0.02, 2.84, -0.62, -4.28, 3.57, -4.7, 2.96, 3.99, 2.47, 3.02, -0.76, -4.7, -1.04, 0.45, -2.07, -4.13, 3.91, -0.51, 2.76, 1.39, -2.25, 4.45, -4.67, 4.05, -1.16, 2.73, -3.98, 0.12, 4.95, 4.55, 0.9, -2.94, 2.55, -2.32, -0.22, -4.05, -3.83, 4.59, -4.0, 0.74, -2.43, 1.47, 0.14, 2.38, -0.48, 0.96, -3.51, -2.69, 0.6, 2.58, -0.66, 2.23, 1.84, -1.57, -0.28, -3.81, 2.01, 3.47, 1.69, -4.81, -4.92, 3.41, 0.9, 2.98, 3.81, -2.2, -2.38, -2.89, 0.12, 3.56, 0.82, 2.16, 0.95, -3.42, -4.22, 1.88, 0.26, -1.28, 0.84, 0.99, 4.58, -3.36, -1.71, -4.29, 4.09, -0.98, -0.07, -2.53, 4.64, -1.41, -1.91, -4.75, 4.96, 0.65, -2.14, 2.95, -2.44, 3.62, -4.75, 1.11, 1.52, 4.04, -4.83, 3.35, 3.93, -3.95, -1.93, 0.89, -0.3, -1.59, -2.04, -1.17, -1.56, 0.48, 3.67, -2.38, 3.0, 4.12, -4.9, 0.27, -4.98, -2.93, -1.34, 1.46, -0.32, -3.01, 0.78, -3.18, -1.91, -4.82, 1.43, -1.97, -4.82, -4.71, 1.39, -2.62, 3.95, -4.7, -3.93, 4.34, 0.75, -0.29, 4.13, -3.28, 3.85, 4.42, 2.14, 3.57, 2.86, 2.11, 2.04, -3.66, -0.6, -0.37, 2.65, 4.62, -3.3, 2.44, 2.41, 1.86, 2.06, 1.85, 0.84, 0.24, 2.53, -2.99, -2.0, 2.23, -0.5, 1.28, 1.74, -1.69, -3.83, 2.55, 1.38, -4.59, -2.49, -2.6, -4.76, -1.68, -0.49, -0.95, -3.67, -4.71, 4.91, -4.97, 4.75, -4.43, -3.38, -3.17, -0.36, -3.2, -4.12, 0.35, -3.27, 2.45, -1.55, 2.86, -0.66, 2.74, -3.22, 1.81, 1.88, 1.35, -0.23, -4.4, 3.85, 1.8, 0.44, -0.9, 1.2, 1.24, 2.94, -1.28, -0.47, 0.63, -3.77, -4.22, 2.6, -3.15, -4.27, 0.57, 3.56, 3.16, 4.06, 1.81, 2.84, 2.51, 2.33, -4.35, 3.65, 3.01, 0.01, -3.65, 1.74, -2.13, 2.18, 0.72, -2.23, 0.23, -4.37, -0.36, 4.16, 3.1, -4.98, 3.82, 2.28, -4.65, 2.11, -1.35, -3.27, 0.49, 4.51, -1.88, 2.65, 4.14, 4.4, -3.34, -4.69, -1.24, -4.11, -0.64, -3.67, 4.58, -0.69, -4.37, -4.6, 2.46, -3.44, 3.47, -4.67, -4.21, 1.52, 3.38, -1.62, 1.81, 2.29, -1.81, -1.04, 1.58, -1.15, 0.83, 3.89, 4.95, 1.72, 2.64, -3.01, -0.58, 4.73, -0.4, 0.12, 1.34, 3.16, -0.44, -4.33, -4.94, 2.88, 4.57, -2.31, -4.77, -0.42, -1.81, -4.89, -0.03, 2.18, 4.14, 4.2, 0.86, -1.4, -2.74, 2.4, -4.37, -3.57, 2.79, 0.58, 2.16, -3.59, 2.66, -2.51, 4.72, -3.32, -3.1, 0.15, -2.61, 4.72, -2.9, -1.5, -0.25, -0.59, -1.67, -2.76, -4.37, -1.18, -3.21, 0.26, 2.73, 2.99, -4.02, -0.81, 2.8, 3.17, 4.17, -0.46, 2.97, 4.6, 1.55, 3.17, -3.65, 0.31, 1.03, -3.74, 3.44, -4.01, 1.75, 4.16, -1.31, 0.23, 2.93, 2.18, -2.78, 3.73, 3.7, 1.89, 3.99, 2.66, -0.83, -1.41, 2.22, -0.63, 3.54, -4.27, -0.25, 2.68, 4.77, 2.13, 4.38, 4.83, -1.72, 4.21, 0.69, -3.21, -3.52, 1.61, 1.47, -3.79, -3.21, -1.49, -4.66, 0.95, 3.67, 3.24, -3.96, 1.01, 4.16, -1.74, -2.16, -1.75, -3.45, 1.26, -4.62, 1.78, 4.9, -3.07, 1.22, 1.68, -0.32, 1.1, -1.93, 0.19, 1.09, 1.88, 4.49, -3.41, 0.66, 1.76, -2.51, 4.14, 0.6, 3.96, -2.41, 3.44, 2.46, 1.84, 3.71, 4.98, 3.81, 1.25, 4.45, 4.1, -4.21, -0.15, 1.05, -3.43, 2.44, 3.62, 2.78, 4.68, 4.94, 0.62, 0.93, -3.84, 4.74, 2.14, -4.58, -2.81, 3.21, -3.38, -1.11, 0.28, -2.94, -3.35, -0.67, -4.64, -4.89, -1.19, -0.42, 0.71, 4.06, 0.08, 2.35, 2.23, -3.28, 3.29, -2.41, 1.99, 3.34, -3.06, 2.56, 2.11, 2.15, -2.75, -2.87, 1.69, -2.88, 3.84, 2.37, 0.44, -1.28, 4.63, -4.24, 4.28, 2.61, 4.26, -1.22, 0.7, -4.48, -4.61, 1.24, -2.7, -2.73, 3.91, -4.92, -1.1, 3.33, 3.96, 0.98, 3.8, 2.27, 1.09, -0.79, -0.92, -3.64, 0.41, 3.76, 4.28, 3.22, -0.52, -2.86, 0.89, 4.5, 0.03, 2.75, 4.24, 2.76, 1.75, -3.17, -2.27, 2.69, 4.57, 4.21, -4.88, -1.5, -3.49, -1.55, -1.81, 4.98, -3.62, -3.18, 1.93, -2.19, 1.13, 4.75, -1.3, -2.05, 0.27, -1.11, 3.6, 0.08, -3.62, 1.06, -1.42, -3.88, -0.73, -1.97, 1.69, -2.33, -3.28, -3.94, -1.31, -3.23, -0.91, -2.82, -0.78, 1.75, -0.03, 1.8, 1.16, -4.3, -1.92, -3.28, -2.89, -2.92, -2.53, 4.62, 1.9, -0.91, -0.34, 4.91, 0.67, -2.87, 4.11, -2.16, 0.64, 0.46, 1.72, -2.2, 2.24, -2.49, 3.5, 3.28, 2.72, 3.02, -3.16, 2.64, -4.39, -1.18, -4.69, -2.73, -2.31, -2.36, 4.4, -2.1, 1.68, -2.3, 0.57, -4.5, -2.27, 2.26, -2.32, -2.48, 4.86, -1.65, 3.95, 0.32, -0.75, 1.94, -1.13, 1.72, 3.85, 2.05, -0.11, 1.59, 4.27, 0.23, 0.61, -3.9, -0.41, -3.0, -2.71, -3.21, -0.59, 1.37, -2.96, -4.34, 1.29, 1.26, 1.02, -3.27, 4.29, -1.55, -1.31, -1.23, 2.52, 4.48, 4.61, -2.33, 4.1, 0.46, 4.06, -3.52, 2.7, -4.97, -3.74, -3.88, 2.69, 0.18, 2.13, -4.78, 1.65, -0.93, -1.64, -2.77, 4.99, -3.98, -2.92, -1.94, 2.85, -3.13, -2.1, -1.11, 3.05, 4.89, -4.03, -4.17, -2.89, -0.7, 4.18, -0.02, -0.26, -1.96, -2.07, -4.2, 1.14, -0.9, 2.1, -3.94, 2.66, -3.56, -3.88, -4.83]\n",
            "------------------------------------\n",
            "Tiempos de ejecucion en CPU: \n",
            "* Tiempo Total:  14.348 [ms]\n",
            "* Tiempo inversion del vector:  0.374 [ms]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Vy5psYHwyD1"
      },
      "source": [
        "## Tabla de pasos:\n",
        "\n",
        "Procesador | Funcion | Detalle\n",
        "---------- | ------- | --------\n",
        "CPU | @param | Lectura de tamaño y limites de los vectores\n",
        "CPU | import | Importacion de las bibliotecas a utilizar\n",
        "CPU | import | Importacion de las bibliotecas a utilizar\n",
        "CPU | import | Importacion de las bibliotecas a utilizar\n",
        "CPU | import | Importacion de las bibliotecas a utilizar\n",
        "CPU | import | Importacion de las bibliotecas a utilizar\n",
        "CPU | import | Importacion de las bibliotecas a utilizar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4t_AJ8TN1RV"
      },
      "source": [
        "## **Ejecucion para GPGPU:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrGnoNUKN5aO"
      },
      "source": [
        "### Instalacion CUDA para Python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JS_dv7gLOAIi",
        "outputId": "4582c00e-f753-4a68-d81a-43130c9d0201"
      },
      "source": [
        "!pip install pycuda"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pycuda\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/61/47d3235a4c13eec5a5f03594ddb268f4858734e02980afbcd806e6242fa5/pycuda-2020.1.tar.gz (1.6MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 8.9MB/s \n",
            "\u001b[?25hCollecting pytools>=2011.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b7/30/c9362a282ef89106768cba9d884f4b2e4f5dc6881d0c19b478d2a710b82b/pytools-2020.4.3.tar.gz (62kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: decorator>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from pycuda) (4.4.2)\n",
            "Collecting appdirs>=1.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/00/2344469e2084fb287c2e0b57b72910309874c3245463acd6cf5e3db69324/appdirs-1.4.4-py2.py3-none-any.whl\n",
            "Collecting mako\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/37/0e706200d22172eb8fa17d68a7ae22dec7631a0a92266634fb518a88a5b2/Mako-1.1.3-py2.py3-none-any.whl (75kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 10.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.8.0 in /usr/local/lib/python3.6/dist-packages (from pytools>=2011.2->pycuda) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from pytools>=2011.2->pycuda) (1.18.5)\n",
            "Requirement already satisfied: dataclasses>=0.7 in /usr/local/lib/python3.6/dist-packages (from pytools>=2011.2->pycuda) (0.7)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from mako->pycuda) (1.1.1)\n",
            "Building wheels for collected packages: pycuda, pytools\n",
            "  Building wheel for pycuda (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycuda: filename=pycuda-2020.1-cp36-cp36m-linux_x86_64.whl size=621008 sha256=480092353675f58b99cd797a4394c0aea70bfcb4e6b35ce6d0692e3b44b9e922\n",
            "  Stored in directory: /root/.cache/pip/wheels/8f/78/d1/5bb826f81d9d490297a348d818ff3ee6dd6f2075b06dde6ea0\n",
            "  Building wheel for pytools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytools: filename=pytools-2020.4.3-py2.py3-none-any.whl size=61374 sha256=b53b30493371e7e1c36a1ec199e975663a7ecd1acef254cc50b25d35c2c7498e\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/c7/81/a22edb90b0b09a880468b2253bb1df8e9f503337ee15432c64\n",
            "Successfully built pycuda pytools\n",
            "Installing collected packages: appdirs, pytools, mako, pycuda\n",
            "Successfully installed appdirs-1.4.4 mako-1.1.3 pycuda-2020.1 pytools-2020.4.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJEYMAcTOD4p"
      },
      "source": [
        "### Ejecucion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLH_WlYKOIok",
        "outputId": "4e6e2442-cb47-48e1-e0bf-527057d27937"
      },
      "source": [
        "#@title Parámetros de ejecución GPGPU { vertical-output: true }\n",
        "# Parametros\n",
        "cantidad_elementos = 1000#@param {type: \"number\"}\n",
        "limite_inferior = -5#@param {type: \"number\"}\n",
        "limite_superior = 5#@param {type: \"number\"}\n",
        "\n",
        "# --------------------------------------------\n",
        "# Importacion de bibliotecas\n",
        "from datetime import datetime\n",
        "tiempo_total = datetime.now()\n",
        "\n",
        "import pycuda.driver as cuda\n",
        "import pycuda.autoinit\n",
        "from pycuda.compiler import SourceModule\n",
        "\n",
        "import numpy\n",
        "\n",
        "# --------------------------------------------\n",
        "# Definición de función que transforma el tiempo en  milisegundos \n",
        "tiempo_en_ms = lambda dt:(dt.days * 24 * 60 * 60 + dt.seconds) * 1000 + dt.microseconds / 1000.0\n",
        "\n",
        "\n",
        "# CPU - Defino la memoria de los vectores en cpu.\n",
        "x_cpu = numpy.random.randn( cantidad_N )\n",
        "#x_cpu = numpy.array([1.00, 2.00, 3.00, 4.00, 5.00, 6.00, 7.00, 8.00, 9.00, 10.00])\n",
        "x_cpu = x_cpu.astype( numpy.float32() )\n",
        "\n",
        "y_cpu = numpy.random.randn( cantidad_N )\n",
        "#y_cpu = numpy.array([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00])\n",
        "y_cpu = y_cpu.astype( numpy.float32() )\n",
        "\n",
        "#tiempo_ini_cpu = datetime.now()\n",
        "\n",
        "r_cpu = numpy.empty_like( x_cpu )\n",
        "\n",
        "# CPU - reservo la memoria GPU.\n",
        "x_gpu = cuda.mem_alloc( x_cpu.nbytes )\n",
        "y_gpu = cuda.mem_alloc( y_cpu.nbytes )\n",
        "\n",
        "# GPU - Copio la memoria al GPU.\n",
        "cuda.memcpy_htod( x_gpu, x_cpu )\n",
        "cuda.memcpy_htod( y_gpu, y_cpu )\n",
        "\n",
        "# CPU - Defino la función kernel que ejecutará en GPU.\n",
        "module = SourceModule(\"\"\"\n",
        "__global__ void kernel_invert( int n, float *X, float *Y )\n",
        "{\n",
        "  int idx = threadIdx.x + blockIdx.x*blockDim.x;\n",
        "  //d_a[idx] = threadIdx.x + 1000*blockIdx.x\n",
        "\n",
        "  if( idx < (n/2) )\n",
        "  {\n",
        "    //Y[idx]  = alfa*X[idx] + Y[idx];\n",
        "    Y[idx] = X[n-idx-1];\n",
        "    Y[n-idx-1] = X[idx];\n",
        "  }\n",
        "}\n",
        "\"\"\") \n",
        "# CPU - Genero la función kernel.\n",
        "kernel = module.get_function(\"kernel_invert\")\n",
        "\n",
        "tiempo_gpu = datetime.now()\n",
        "\n",
        "# GPU - Ejecuta el kernel.\n",
        "# TODO: Falta consultar limites del GPU, para armar las dimensiones correctamente.\n",
        "dim_hilo = 256\n",
        "dim_bloque = numpy.int( (cantidad_N+dim_hilo-1) / dim_hilo )\n",
        "print( \"Thread x: \", dim_hilo, \", Bloque x:\", dim_bloque )\n",
        "\n",
        "#TODO: Ojo, con los tipos de las variables en el kernel.\n",
        "kernel( numpy.int32(cantidad_N), x_gpu, y_gpu, block=( dim_hilo, 1, 1 ),grid=(dim_bloque, 1,1) )\n",
        "\n",
        "tiempo_gpu = datetime.now() - tiempo_gpu\n",
        "\n",
        "# GPU - Copio el resultado desde la memoria GPU.\n",
        "cuda.memcpy_dtoh( r_cpu, y_gpu )\n",
        "\n",
        "#Redondeo los vectores para mostrarlos\n",
        "x_cpu_rounded = [round(x,2) for x in x_cpu]\n",
        "y_cpu_rounded = [round(x,2) for x in y_cpu]\n",
        "r_cpu_rounded = [round(x,2) for x in r_cpu]\n",
        "\n",
        "# CPU - Informo el resutlado.\n",
        "print( \"------------------------------------\")\n",
        "print( \"X: \" )\n",
        "print( x_cpu_rounded )\n",
        "print( \"------------------------------------\")\n",
        "print( \"Y: \" )\n",
        "print( y_cpu_rounded )\n",
        "print( \"------------------------------------\")\n",
        "print( \"R: \" )\n",
        "print( r_cpu_rounded )\n",
        "\n",
        "\n",
        "tiempo_total = datetime.now() - tiempo_total\n",
        "\n",
        "print( \"Cantidad de elementos: \", cantidad_N )\n",
        "print( \"Thread x: \", dim_hilo, \", Bloque x:\", dim_bloque )\n",
        "print(\"Tiempo CPU: \", tiempo_en_ms( tiempo_total ), \"[ms]\" )\n",
        "print(\"Tiempo GPU: \", tiempo_en_ms( tiempo_gpu   ), \"[ms]\" )"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thread x:  256 , Bloque x: 1\n",
            "------------------------------------\n",
            "X: \n",
            "[0.34, -0.21, 1.46, -0.87, 0.87, 1.51, -0.22, 1.15, 1.51, 1.11, -2.22, -0.42, 1.11, -0.13, -0.24, 0.41, -0.83, 0.7, 0.86, 1.93, 1.23, 2.13, -1.81, 0.18, 0.16, -0.06, -0.35, 0.08, 1.2, -0.68, 1.9, 1.05, -0.83, -0.47, 1.54, 1.39, -0.75, -0.99, 0.2, -2.29, -0.9, -0.91, 0.27, 1.65, -1.69, -1.99, -0.37, -0.35, -0.54, -0.82, -0.6, 0.57, 0.66, 0.94, 1.31, 0.58, -0.23, -1.09, -0.01, -0.05, -0.73, 1.33, -0.21, 0.14, 0.44, 0.37, -1.14, 0.11, -2.15, 0.89, 1.45, -0.09, -0.81, -0.36, 0.98, -1.98, 1.09, 2.37, -0.16, -0.17, -2.13, 0.49, 0.31, 0.23, -0.28, -0.8, 1.58, 0.47, 0.18, 0.67, 0.34, 0.19, -1.64, -0.28, 0.26, 0.97, -0.88, 1.08, 0.37, 0.56]\n",
            "------------------------------------\n",
            "Y: \n",
            "[0.55, 0.32, 0.45, -0.08, -0.63, 1.08, 0.16, 0.03, -1.1, -0.06, -0.12, -0.75, -0.17, -1.24, 0.1, -1.59, -0.31, 0.87, 1.17, 0.97, 1.67, -0.94, 2.08, 1.68, 1.14, 0.2, -0.3, -0.06, -0.33, -0.03, -1.5, -0.64, 0.13, 1.65, 0.17, -1.22, 0.15, -0.32, -0.82, 1.17, -0.12, 0.86, -0.24, 1.02, 1.05, 0.17, 1.9, -0.41, -0.53, -0.35, 0.66, -0.04, 0.18, -0.67, 0.46, -1.01, 0.33, -0.61, -0.55, -2.11, -0.12, -0.17, 0.68, 0.37, -0.21, -2.34, -0.33, 0.12, 0.8, -0.33, -1.54, 2.18, -0.34, 2.6, -0.07, 0.15, 1.86, 0.52, 0.27, 0.15, 0.28, -1.07, 2.06, -1.98, 0.62, 0.11, 0.76, -0.1, 0.61, 0.42, -1.21, -1.33, 0.6, 0.71, -1.71, 0.97, 0.18, 0.3, 0.4, 0.85]\n",
            "------------------------------------\n",
            "R: \n",
            "[0.56, 0.37, 1.08, -0.88, 0.97, 0.26, -0.28, -1.64, 0.19, 0.34, 0.67, 0.18, 0.47, 1.58, -0.8, -0.28, 0.23, 0.31, 0.49, -2.13, -0.17, -0.16, 2.37, 1.09, -1.98, 0.98, -0.36, -0.81, -0.09, 1.45, 0.89, -2.15, 0.11, -1.14, 0.37, 0.44, 0.14, -0.21, 1.33, -0.73, -0.05, -0.01, -1.09, -0.23, 0.58, 1.31, 0.94, 0.66, 0.57, -0.6, -0.82, -0.54, -0.35, -0.37, -1.99, -1.69, 1.65, 0.27, -0.91, -0.9, -2.29, 0.2, -0.99, -0.75, 1.39, 1.54, -0.47, -0.83, 1.05, 1.9, -0.68, 1.2, 0.08, -0.35, -0.06, 0.16, 0.18, -1.81, 2.13, 1.23, 1.93, 0.86, 0.7, -0.83, 0.41, -0.24, -0.13, 1.11, -0.42, -2.22, 1.11, 1.51, 1.15, -0.22, 1.51, 0.87, -0.87, 1.46, -0.21, 0.34]\n",
            "Cantidad de elementos:  100\n",
            "Thread x:  256 , Bloque x: 1\n",
            "Tiempo CPU:  258.939 [ms]\n",
            "Tiempo GPU:  0.545 [ms]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}