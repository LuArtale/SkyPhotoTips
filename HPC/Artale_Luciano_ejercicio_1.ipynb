{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Artale_Luciano_ejercicio_1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMEcqgMTS9amNowTgNPrOwD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LuArtale/SkyPhotoTips/blob/master/HPC/Artale_Luciano_ejercicio_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgCUzYyKMF4S"
      },
      "source": [
        "# **Ejercicio 1: Invertir Vector**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdjakVuWXh7U"
      },
      "source": [
        "## Introduccion:\n",
        "\n",
        "En este cuaderno se demuestran las diferencias de ejecucion para una funcion que tiene como objetivo la inversion de un vector utilizando primero la ejecucion en CPU con python y luego la ejecucion en GPGPU con python y CUDA."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4yWZ-bLbMPM"
      },
      "source": [
        "## Armado del ambiente:\n",
        "\n",
        "Para poder ejecutar el ejercicio en CPU no se debe ejecutar ningun comando previo, pero para ejecutarlo en GPGPU se debe correr la seccion llamada \"Instalacion CUDA para Python\". \n",
        "Para ambos casos, se debe indicar como parametro la cantidad de elementos del vector a invertir (los mismos son numeros decimales generados de forma aleatoria) y el valor maximo y minimo que pueden tomar esos elementos.\n",
        "\n",
        "Ademas de esto, para CPU se debe seleccionar el entorno de ejecucion \"None\" y para GPGPU se debe seleccionar el entorno \"GPU\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zY7bXCT6Nu7a"
      },
      "source": [
        "## **Ejecucion para CPU:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qEjDLGTIr83",
        "outputId": "27849cea-224a-46a0-d6db-92b495b5ba9d"
      },
      "source": [
        "#@title Parámetros de ejecución CPU { vertical-output: true }\n",
        "# Parametros\n",
        "cantidad_elementos = 100#@param {type: \"number\"}\n",
        "limite_inferior = -5#@param {type: \"number\"}\n",
        "limite_superior = 5#@param {type: \"number\"}\n",
        "\n",
        "# --------------------------------------------\n",
        "# Importacion de bibliotecas\n",
        "from datetime import datetime\n",
        "tiempo_total = datetime.now()\n",
        "import numpy\n",
        "\n",
        "# --------------------------------------------\n",
        "# Definición de función que transforma el tiempo en  milisegundos \n",
        "tiempo_en_ms = lambda dt:(dt.days * 24 * 60 * 60 + dt.seconds) * 1000 + dt.microseconds / 1000.0\n",
        "\n",
        "# --------------------------------------------\n",
        "# Defino la memoria de los vectores en cpu\n",
        "vector_original_cpu = numpy.random.uniform( limite_inferior, limite_superior, cantidad_elementos )\n",
        "vector_original_cpu = vector_original_cpu.astype( numpy.float32() )\n",
        "\n",
        "# El vector resultado (invertido) se define vacio\n",
        "vector_invertido_cpu = numpy.empty_like( vector_original_cpu )\n",
        "\n",
        "# --------------------------------------------\n",
        "# Realizo la inversion de vectores\n",
        "\n",
        "tiempo_bucle_inversion = datetime.now()\n",
        "\n",
        "# Solo se recorre hasta la mitad del vector ya que se procesan 2 posiciones a la vez\n",
        "cantidad_a_recorrer = cantidad_elementos // 2\n",
        "\n",
        "# Bucle de inversion\n",
        "for idx in range( 0, cantidad_a_recorrer ):\n",
        "  vector_invertido_cpu[idx] = vector_original_cpu[cantidad_elementos-idx-1]\n",
        "  vector_invertido_cpu[cantidad_elementos-idx-1] = vector_original_cpu[idx]\n",
        "\n",
        "tiempo_bucle_inversion = datetime.now() - tiempo_bucle_inversion\n",
        "\n",
        "# --------------------------------------------\n",
        "# Redondeo los elementos de los vectores a dos decimales\n",
        "vector_original_cpu_rounded = [round(x,2) for x in vector_original_cpu]\n",
        "vector_invertido_cpu_rounded = [round(x,2) for x in vector_invertido_cpu]\n",
        "\n",
        "# Informo los resultados\n",
        "print( \"Cantidad de elementos: \", cantidad_elementos )\n",
        "print( \"------------------------------------\")\n",
        "\n",
        "print(\"Vectores: \")\n",
        "print( \"* Vector Original: \" )\n",
        "print( vector_original_cpu_rounded )\n",
        "print( \"------------------------------------\")\n",
        "print( \"* Vector Invertido: \" )\n",
        "print( vector_invertido_cpu_rounded )\n",
        "print( \"------------------------------------\")\n",
        "\n",
        "tiempo_total = datetime.now() - tiempo_total\n",
        "\n",
        "print(\"Tiempos de ejecucion en CPU: \")\n",
        "print(\"* Tiempo Total: \", tiempo_en_ms( tiempo_total ), \"[ms]\" )\n",
        "print(\"* Tiempo inversion del vector: \", tiempo_en_ms( tiempo_bucle_inversion ), \"[ms]\" )"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cantidad de elementos:  100\n",
            "------------------------------------\n",
            "Vectores: \n",
            "* Vector Original: \n",
            "[2.58, -2.03, 2.97, 2.88, -4.63, 0.92, -4.48, 2.1, 4.56, -5.0, -4.41, 0.98, -2.72, -2.83, 1.26, -4.33, -2.44, 0.49, 3.71, 0.61, 4.47, 0.84, 0.87, 0.55, 0.64, -2.03, -1.03, 2.66, 2.0, 2.67, -3.61, -1.8, 3.4, -1.42, -4.99, -4.16, 3.23, -2.16, -1.22, -1.57, 0.44, -1.44, -0.05, -4.4, 4.48, -2.01, -0.38, 3.75, 0.45, -1.09, 3.27, 2.22, 4.97, -0.77, -4.87, 1.49, -3.72, -1.25, 1.42, 3.06, -1.6, -2.26, 1.7, 3.84, 2.06, -3.33, 4.68, -1.06, 4.4, 2.57, -4.01, 4.03, -4.25, -3.44, 3.57, -2.23, 1.39, 3.66, -0.63, -4.72, -3.27, -3.05, -3.82, -0.93, -1.1, -0.47, -1.64, -1.72, -0.1, 0.85, -2.73, -2.13, -1.22, -4.72, 4.58, 4.84, 1.96, -2.58, -3.74, 3.37]\n",
            "------------------------------------\n",
            "* Vector Invertido: \n",
            "[3.37, -3.74, -2.58, 1.96, 4.84, 4.58, -4.72, -1.22, -2.13, -2.73, 0.85, -0.1, -1.72, -1.64, -0.47, -1.1, -0.93, -3.82, -3.05, -3.27, -4.72, -0.63, 3.66, 1.39, -2.23, 3.57, -3.44, -4.25, 4.03, -4.01, 2.57, 4.4, -1.06, 4.68, -3.33, 2.06, 3.84, 1.7, -2.26, -1.6, 3.06, 1.42, -1.25, -3.72, 1.49, -4.87, -0.77, 4.97, 2.22, 3.27, -1.09, 0.45, 3.75, -0.38, -2.01, 4.48, -4.4, -0.05, -1.44, 0.44, -1.57, -1.22, -2.16, 3.23, -4.16, -4.99, -1.42, 3.4, -1.8, -3.61, 2.67, 2.0, 2.66, -1.03, -2.03, 0.64, 0.55, 0.87, 0.84, 4.47, 0.61, 3.71, 0.49, -2.44, -4.33, 1.26, -2.83, -2.72, 0.98, -4.41, -5.0, 4.56, 2.1, -4.48, 0.92, -4.63, 2.88, 2.97, -2.03, 2.58]\n",
            "------------------------------------\n",
            "Tiempos de ejecucion en CPU: \n",
            "* Tiempo Total:  3.146 [ms]\n",
            "* Tiempo inversion del vector:  0.196 [ms]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Vy5psYHwyD1"
      },
      "source": [
        "## Tabla de pasos:\n",
        "\n",
        "Procesador | Funcion | Detalle\n",
        "---------- | ------- | --------\n",
        "CPU | @param | Lectura de tamaño y limites de los vectores\n",
        "CPU | import | Importacion de las bibliotecas a utilizar\n",
        "CPU | datetime.now() | Se lee el tiempo inicial total\n",
        "CPU | numpy.random.uniform() | Creacion de los vectores y asignacion de su memoria\n",
        "CPU | datetime.now() | Se lee el tiempo inicial del bucle\n",
        "CPU | for | Inversion de un vector copiandose en el otro\n",
        "CPU | datetime.now() | Se lee el tiempo final del bucle\n",
        "CPU | round() | Redondeo de los elementos de los vectores\n",
        "CPU | datetime.now() | Se lee el tiempo final total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4t_AJ8TN1RV"
      },
      "source": [
        "## **Ejecucion para GPGPU:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrGnoNUKN5aO"
      },
      "source": [
        "### Instalacion CUDA para Python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JS_dv7gLOAIi",
        "outputId": "f647310a-8c37-4091-8c60-46c3e22d3142"
      },
      "source": [
        "!pip install pycuda"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pycuda\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/61/47d3235a4c13eec5a5f03594ddb268f4858734e02980afbcd806e6242fa5/pycuda-2020.1.tar.gz (1.6MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 18.3MB/s \n",
            "\u001b[?25hCollecting pytools>=2011.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b7/30/c9362a282ef89106768cba9d884f4b2e4f5dc6881d0c19b478d2a710b82b/pytools-2020.4.3.tar.gz (62kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 11.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: decorator>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from pycuda) (4.4.2)\n",
            "Collecting appdirs>=1.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/00/2344469e2084fb287c2e0b57b72910309874c3245463acd6cf5e3db69324/appdirs-1.4.4-py2.py3-none-any.whl\n",
            "Collecting mako\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/37/0e706200d22172eb8fa17d68a7ae22dec7631a0a92266634fb518a88a5b2/Mako-1.1.3-py2.py3-none-any.whl (75kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 11.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.8.0 in /usr/local/lib/python3.6/dist-packages (from pytools>=2011.2->pycuda) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from pytools>=2011.2->pycuda) (1.18.5)\n",
            "Requirement already satisfied: dataclasses>=0.7 in /usr/local/lib/python3.6/dist-packages (from pytools>=2011.2->pycuda) (0.7)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from mako->pycuda) (1.1.1)\n",
            "Building wheels for collected packages: pycuda, pytools\n",
            "  Building wheel for pycuda (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycuda: filename=pycuda-2020.1-cp36-cp36m-linux_x86_64.whl size=621008 sha256=eb896e705cbd3f95510334e86a8e5c21a4067965da7e976e6e55a83104d65354\n",
            "  Stored in directory: /root/.cache/pip/wheels/8f/78/d1/5bb826f81d9d490297a348d818ff3ee6dd6f2075b06dde6ea0\n",
            "  Building wheel for pytools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytools: filename=pytools-2020.4.3-py2.py3-none-any.whl size=61374 sha256=f1d8f4c7839fead2e31bdfbfd4d263b1819413a647a362999dda4ef0e5ca0700\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/c7/81/a22edb90b0b09a880468b2253bb1df8e9f503337ee15432c64\n",
            "Successfully built pycuda pytools\n",
            "Installing collected packages: appdirs, pytools, mako, pycuda\n",
            "Successfully installed appdirs-1.4.4 mako-1.1.3 pycuda-2020.1 pytools-2020.4.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJEYMAcTOD4p"
      },
      "source": [
        "### Ejecucion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLH_WlYKOIok",
        "outputId": "02e6dd22-5805-4dd9-869c-171a7d2933d8"
      },
      "source": [
        "#@title Parámetros de ejecución GPGPU { vertical-output: true }\n",
        "# Parametros\n",
        "cantidad_elementos = 100#@param {type: \"number\"}\n",
        "limite_inferior = -5#@param {type: \"number\"}\n",
        "limite_superior = 5#@param {type: \"number\"}\n",
        "\n",
        "# --------------------------------------------\n",
        "# Importacion de bibliotecas\n",
        "from datetime import datetime\n",
        "tiempo_total = datetime.now()\n",
        "\n",
        "import pycuda.driver as cuda\n",
        "import pycuda.autoinit\n",
        "from pycuda.compiler import SourceModule\n",
        "\n",
        "import numpy\n",
        "\n",
        "# --------------------------------------------\n",
        "# Definición de función que transforma el tiempo en  milisegundos \n",
        "tiempo_en_ms = lambda dt:(dt.days * 24 * 60 * 60 + dt.seconds) * 1000 + dt.microseconds / 1000.0\n",
        "\n",
        "\n",
        "# --------------------------------------------\n",
        "# Defino la memoria de los vectores en cpu\n",
        "vector_original_cpu = numpy.random.uniform( limite_inferior, limite_superior, cantidad_elementos )\n",
        "vector_original_cpu = vector_original_cpu.astype( numpy.float32() )\n",
        "\n",
        "# El vector resultado (invertido) se define vacio\n",
        "vector_invertido_cpu = numpy.empty_like( vector_original_cpu )\n",
        "\n",
        "# CPU - Reservo la memoria GPU.\n",
        "vector_original_gpu = cuda.mem_alloc( vector_original_cpu.nbytes )\n",
        "vector_invertido_gpu = cuda.mem_alloc( vector_invertido_cpu.nbytes )\n",
        "\n",
        "# GPU - Copio la memoria al GPU.\n",
        "cuda.memcpy_htod( vector_original_gpu, vector_original_cpu )\n",
        "cuda.memcpy_htod( vector_invertido_gpu, vector_invertido_cpu )\n",
        "\n",
        "# CPU - Defino la función kernel que ejecutará en GPU\n",
        "module = SourceModule(\"\"\"\n",
        "__global__ void kernel_invert( int cant_elementos, float *vec_original, float *vec_invertido )\n",
        "{\n",
        "  int idx = threadIdx.x + blockIdx.x*blockDim.x;\n",
        "  int cantidad_a_recorrer = cant_elementos / 2;\n",
        "\n",
        "  if( idx < cantidad_a_recorrer )\n",
        "  {\n",
        "    vec_invertido[idx] = vec_original[cant_elementos-idx-1];\n",
        "    vec_invertido[cant_elementos-idx-1] = vec_original[idx];\n",
        "  }\n",
        "}\n",
        "\"\"\") \n",
        "# CPU - Genero la función kernel.\n",
        "kernel = module.get_function(\"kernel_invert\")\n",
        "\n",
        "tiempo_gpu = datetime.now()\n",
        "\n",
        "# GPU - Ejecuta el kernel.\n",
        "# TODO: Falta consultar limites del GPU, para armar las dimensiones correctamente.\n",
        "dim_hilo = 256 \n",
        "dim_bloque = numpy.int( (cantidad_elementos+dim_hilo-1) / dim_hilo )\n",
        "print( \"Thread x: \", dim_hilo, \", Bloque x:\", dim_bloque )\n",
        "print( \"Cantidad de elementos: \", cantidad_elementos )\n",
        "print( \"------------------------------------\")\n",
        "\n",
        "# Llamo a ejecutar el kernel con el codigo declarado arriba\n",
        "kernel( numpy.int32(cantidad_elementos), vector_original_gpu, vector_invertido_gpu, block=( dim_hilo, 1, 1 ),grid=(dim_bloque, 1,1) )\n",
        "\n",
        "tiempo_gpu = datetime.now() - tiempo_gpu\n",
        "\n",
        "# GPU - Copio el resultado desde la memoria GPU.\n",
        "cuda.memcpy_dtoh( vector_invertido_cpu, vector_invertido_gpu )\n",
        "\n",
        "# Redondeo los vectores para mostrarlos\n",
        "vector_original_cpu_rounded = [round(x,2) for x in vector_original_cpu]\n",
        "vector_invertido_cpu_rounded = [round(x,2) for x in vector_invertido_cpu]\n",
        "\n",
        "# Informo los resultados\n",
        "print(\"Vectores: \")\n",
        "print( \"* Vector Original: \" )\n",
        "print( vector_original_cpu_rounded )\n",
        "print( \"------------------------------------\")\n",
        "print( \"* Vector Invertido: \" )\n",
        "print( vector_invertido_cpu_rounded )\n",
        "print( \"------------------------------------\")\n",
        "\n",
        "tiempo_total = datetime.now() - tiempo_total\n",
        "\n",
        "print(\"Tiempos de ejecucion: \")\n",
        "print(\"* Tiempo Total: \", tiempo_en_ms( tiempo_total ), \"[ms]\" )\n",
        "print(\"* Tiempo GPU: \", tiempo_en_ms( tiempo_gpu   ), \"[ms]\" )"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thread x:  256 , Bloque x: 4\n",
            "Cantidad de elementos:  1000\n",
            "------------------------------------\n",
            "Vectores: \n",
            "* Vector Original: \n",
            "[-0.98, -4.73, -2.12, -3.59, -2.3, 1.45, 2.69, -2.04, -0.06, 3.61, -1.51, 1.48, 1.41, 4.04, 1.02, 2.82, -0.11, -2.45, 0.66, 0.63, 1.41, -0.71, -3.1, -0.84, -0.6, 3.56, 0.86, 4.4, 0.33, 3.33, 3.54, -0.22, -2.12, -4.13, -2.27, 4.01, -3.5, 2.38, -0.84, -0.26, -1.47, -3.77, -1.62, -3.62, 2.54, 0.5, 2.07, 4.85, -4.16, -4.14, -2.4, 4.02, -1.51, -3.38, 3.19, 2.54, -4.31, 4.85, -4.27, -1.53, 1.49, 3.03, -4.8, 0.37, -0.13, 4.58, 0.68, 2.94, 0.83, 3.96, -2.21, 0.97, 1.09, 4.0, -0.98, 2.92, -0.1, 3.34, -2.42, -4.82, 4.01, 1.2, 1.17, -4.13, 1.99, 2.8, 2.03, 3.04, -3.92, -4.79, 3.32, 1.27, -2.76, -2.42, -0.74, 1.72, -2.19, 3.01, -4.16, -0.34, -2.72, 2.16, 3.81, 3.61, -3.62, 4.56, 4.51, 3.04, -3.04, 0.36, 0.51, 3.62, 2.83, 4.99, 1.29, 4.22, 2.95, 2.92, 5.0, 3.63, -3.02, 1.75, 3.77, -3.04, 3.75, 4.59, 2.45, 2.37, 1.51, 3.68, 3.12, 4.94, 0.04, -2.46, -0.41, 0.41, -4.28, 1.81, -4.32, -0.42, 1.13, -1.69, 4.62, 2.84, -2.03, 2.21, -1.82, -0.19, 2.86, 3.2, 0.1, -1.95, -1.32, 3.42, 4.96, -3.66, 3.15, -4.71, -0.54, -3.65, 3.95, 1.0, 3.01, 1.54, 4.69, 1.17, 2.44, 1.75, 4.18, 3.15, 3.8, 0.96, -1.43, -0.36, 1.42, -1.51, -0.54, 4.9, -4.19, 1.53, 1.97, 4.76, 3.92, 0.36, 4.06, 1.55, 1.17, 4.05, 2.37, 2.73, 1.18, 0.95, 0.53, -1.34, -3.43, 0.06, 3.04, 4.95, -0.76, 0.65, -2.0, 4.05, -0.16, 2.86, 2.43, 3.84, 2.54, 1.41, -3.42, 3.29, -2.56, -1.47, 0.52, 3.07, -4.59, 3.63, -1.74, 1.1, 1.91, 1.54, -2.96, 1.22, -1.33, -0.41, 2.78, -2.75, -0.97, 1.72, -1.91, -0.53, -3.18, -1.53, 1.72, 0.67, 1.09, 2.7, -0.01, 1.22, -0.82, -2.15, -1.23, 1.78, 0.7, 1.01, 2.1, -4.52, -3.74, -2.63, 3.39, -3.91, 0.77, -0.06, -1.45, -1.15, -0.75, -4.71, -2.72, -0.13, -2.72, 2.68, -4.4, 1.12, 1.28, 3.42, -3.75, 2.86, -4.41, 0.86, 2.11, -1.22, 1.08, 2.11, -5.0, -1.31, -3.58, 2.73, -1.13, -3.25, -0.3, -4.49, -1.45, -0.34, 4.63, -0.9, -2.36, -4.42, -0.55, 3.06, 0.68, -1.54, 1.1, 3.8, -0.44, 3.88, -4.4, -3.28, -2.16, 3.24, 4.13, 1.58, 0.09, -1.14, 2.44, -1.36, 4.05, 1.35, 1.86, 0.84, 0.71, -2.48, -0.9, 4.3, 1.35, -4.17, 1.98, -4.11, 1.7, -3.64, 4.62, 0.36, -2.04, -0.77, 4.22, -1.24, 1.98, -0.17, -1.62, 0.44, -4.22, -0.46, -4.78, 2.05, -3.36, -2.52, -1.93, -0.77, -1.56, -2.88, 4.03, -3.2, -0.78, -0.49, 4.67, -0.21, -1.59, -2.76, 4.02, 3.2, 3.32, 1.92, 4.09, -0.78, -3.79, 4.82, -1.5, 3.04, 0.52, 0.89, -1.35, 3.43, 2.23, -1.47, -0.88, 2.23, 3.8, -0.27, -3.99, 0.3, 2.61, 4.93, 0.55, 1.11, -1.39, 2.07, -1.13, -0.56, -2.65, 1.77, 0.53, -1.19, 1.0, -1.32, 1.04, -0.1, 0.62, -4.94, -3.38, 0.79, -3.95, -4.74, 3.88, -4.06, -3.22, 2.07, -0.77, 4.88, 2.51, 3.82, 3.34, 1.73, 2.28, 3.1, 1.75, -4.45, -3.39, -4.12, -3.1, -4.65, 1.61, -1.45, -4.48, 3.3, -0.29, 1.13, -4.18, 3.85, -1.43, -3.19, -1.61, 2.27, -1.18, -1.54, 0.4, -3.33, 4.47, 0.26, 4.34, 1.41, -0.98, 2.83, 3.76, 3.69, -3.76, 4.94, -1.1, 0.49, -2.72, -0.49, -3.83, 3.32, -2.4, -3.22, -2.71, 2.81, -2.69, -1.03, -4.12, -3.12, -0.08, -1.25, -1.36, 0.36, 5.0, 0.35, -2.12, 0.53, 1.19, -0.36, -3.35, -2.06, 2.32, -3.36, -2.68, -4.23, -1.22, -3.06, -4.99, -3.12, -1.35, 1.26, -4.51, -4.36, 4.9, -3.71, 0.55, 3.27, -2.92, -1.6, -0.47, 4.04, -0.62, 4.89, 0.54, 4.23, -2.24, -3.56, -0.77, 0.02, -2.73, -0.38, 3.89, 0.73, -2.41, -2.63, 0.92, 1.48, -2.99, -3.72, 0.19, 0.84, -2.98, 4.06, -4.46, -3.18, -0.26, 2.88, -2.81, 4.73, -0.67, 3.17, -4.55, -2.58, 4.18, 4.12, 1.81, 3.22, -1.32, 4.98, -4.57, 2.19, 1.14, 3.67, 2.34, -0.77, -2.54, -1.36, -1.53, 4.95, 0.3, -2.5, -4.02, 0.13, 1.4, -0.09, 2.19, 4.75, -4.06, 4.51, -1.5, 3.86, -1.08, 2.65, 2.76, -2.48, 4.96, -2.6, 3.81, 2.81, 4.24, 4.71, -3.82, -2.68, 0.08, -4.94, -0.81, -1.32, 3.23, 2.08, -1.32, -2.06, 3.86, 1.74, 3.06, -1.52, -2.13, -4.78, 1.74, -2.48, -1.51, -3.08, -0.34, -2.56, -1.76, -0.57, -2.15, -4.65, -1.43, -3.82, -0.83, -4.99, -4.21, 3.32, -1.15, -0.47, 1.42, -3.06, -2.46, 0.84, -1.81, -1.78, -2.26, -0.22, -0.29, 0.39, 2.15, -4.47, 1.49, 4.96, -1.1, -0.1, -2.99, 3.54, -4.29, 1.92, 0.05, -4.94, -4.14, 0.73, -0.01, -3.6, 2.11, 1.76, 3.4, -0.74, -1.37, -1.69, 1.78, -3.52, -4.53, 4.84, -4.12, -4.58, 3.11, -0.17, 1.33, -1.03, -2.68, -4.58, 1.28, 0.74, -4.0, 4.25, -4.28, 3.5, 4.98, -3.71, -0.64, -3.08, 3.61, -0.59, 4.28, -3.22, 0.65, 2.9, 2.41, -4.61, -4.46, 2.12, 1.84, 3.24, 3.1, -4.01, 2.51, 3.37, 1.54, -3.89, 1.16, -0.45, 2.12, 4.67, -0.9, -3.9, -2.0, 0.12, -0.05, -0.17, 1.78, -2.92, 2.14, -0.63, 3.4, 4.57, -1.45, -0.46, 1.95, 1.63, 4.68, -2.25, -0.09, 0.29, -4.38, 2.35, -1.33, -3.66, -0.84, -3.04, -4.17, 4.63, -0.19, 0.57, 4.26, -3.57, -2.99, -0.19, 2.94, 0.74, -1.15, -0.04, -2.13, 1.57, -4.83, 1.91, -0.17, 2.8, 1.35, 1.92, -4.72, 1.35, 0.9, -3.68, -0.18, 0.41, 2.72, 2.45, 3.6, -3.41, -0.49, -4.9, 2.73, -4.72, 2.0, -0.73, 4.7, 4.12, 3.27, 3.66, 0.01, 0.87, -1.45, 2.91, 1.99, 1.46, -2.08, -4.09, 0.74, 3.88, 1.03, -2.83, -3.16, 1.4, 1.71, -4.89, -4.56, 2.61, -4.85, -3.51, 4.25, -0.87, 3.7, -0.36, -1.82, -2.41, -1.45, -3.65, -4.05, -1.31, -0.38, -4.15, 0.12, -0.06, 1.2, -1.19, -4.63, 1.95, 0.52, 0.36, 2.47, 2.73, 3.75, 0.72, -2.72, -4.56, 1.64, -0.82, 1.64, 0.7, 0.91, -0.46, 3.05, -4.66, -2.83, 3.1, 3.68, 2.78, 4.03, -2.51, 4.88, -1.89, -3.89, 3.64, 4.7, -2.31, -2.0, 3.94, 0.87, -1.44, 4.09, -3.72, 2.26, -4.33, 4.64, -2.71, 4.89, 4.27, -1.03, -3.76, -3.67, 0.13, -0.21, 2.67, -3.72, -4.03, -4.06, 0.46, 4.79, -2.51, -4.23, -4.98, 4.29, -0.17, -3.91, 1.2, 0.09, -1.88, 0.15, -2.2, 0.25, 0.48, -0.49, -2.5, -2.3, -1.87, -1.65, 0.16, 4.86, 3.76, 4.62, 1.38, 3.66, 1.91, 3.13, -0.44, -3.29, 4.55, -1.14, -1.19, 2.88, -1.34, 0.73, 2.79, -3.64, 0.75, -0.74, -0.58, -3.61, 3.26, -0.07, 1.03, -2.08, 2.13, 1.9, 1.33, 1.56, 2.94, -2.18, -1.34, 2.64, -1.05, 2.21, 3.39, -2.11, 2.19, -3.02, 0.82, -2.8, 0.89, 0.39, -0.74, 1.64, 0.83, 1.14, -2.07, 0.76, -3.65, -4.0, 1.11, -2.58, 1.5, 4.57, 2.88, -4.2, 1.11, 0.77, -1.74, -4.56, 2.44, 4.21, -2.61, -1.28, -4.43, 4.39, 2.03, -0.27, 0.62, -0.66, 2.75, 1.66, 4.81, -4.25, 2.18, 0.34, -4.21, -0.47, -2.79, 0.91, -4.41, -2.05, 1.38, 3.35, -0.26, 2.33, 0.4, 0.34, -2.28, -0.84, -2.36, -1.23, -2.66, -4.78, 0.04, -1.7, -2.69, 2.87, 2.9, 0.57, 1.91, -0.07, -1.69, -4.33, -3.76, -0.5, 4.55, -3.38, 3.37, -4.89, 1.62, 3.52, -2.75, -2.22, 3.53, 1.34, -0.71, 0.16, 1.91, 2.54, -4.01, 2.87, 0.51, -0.54, 2.45, 3.74, 4.36, 2.33, 3.78, 4.42, 3.53, 0.16, -1.15, 2.18, 2.75, 2.15, -0.36, -4.23, -4.29, 2.32, -3.41, 0.75, 3.21, -1.68, 4.74, 3.14, 3.31, 1.61, -2.65, 2.62, -3.6, -4.68, 2.73, 0.25, 3.55, 4.94, 2.1, 2.93, 3.2, -2.6, -1.67, 2.79, 2.69, 3.01, -2.08]\n",
            "------------------------------------\n",
            "* Vector Invertido: \n",
            "[-2.08, 3.01, 2.69, 2.79, -1.67, -2.6, 3.2, 2.93, 2.1, 4.94, 3.55, 0.25, 2.73, -4.68, -3.6, 2.62, -2.65, 1.61, 3.31, 3.14, 4.74, -1.68, 3.21, 0.75, -3.41, 2.32, -4.29, -4.23, -0.36, 2.15, 2.75, 2.18, -1.15, 0.16, 3.53, 4.42, 3.78, 2.33, 4.36, 3.74, 2.45, -0.54, 0.51, 2.87, -4.01, 2.54, 1.91, 0.16, -0.71, 1.34, 3.53, -2.22, -2.75, 3.52, 1.62, -4.89, 3.37, -3.38, 4.55, -0.5, -3.76, -4.33, -1.69, -0.07, 1.91, 0.57, 2.9, 2.87, -2.69, -1.7, 0.04, -4.78, -2.66, -1.23, -2.36, -0.84, -2.28, 0.34, 0.4, 2.33, -0.26, 3.35, 1.38, -2.05, -4.41, 0.91, -2.79, -0.47, -4.21, 0.34, 2.18, -4.25, 4.81, 1.66, 2.75, -0.66, 0.62, -0.27, 2.03, 4.39, -4.43, -1.28, -2.61, 4.21, 2.44, -4.56, -1.74, 0.77, 1.11, -4.2, 2.88, 4.57, 1.5, -2.58, 1.11, -4.0, -3.65, 0.76, -2.07, 1.14, 0.83, 1.64, -0.74, 0.39, 0.89, -2.8, 0.82, -3.02, 2.19, -2.11, 3.39, 2.21, -1.05, 2.64, -1.34, -2.18, 2.94, 1.56, 1.33, 1.9, 2.13, -2.08, 1.03, -0.07, 3.26, -3.61, -0.58, -0.74, 0.75, -3.64, 2.79, 0.73, -1.34, 2.88, -1.19, -1.14, 4.55, -3.29, -0.44, 3.13, 1.91, 3.66, 1.38, 4.62, 3.76, 4.86, 0.16, -1.65, -1.87, -2.3, -2.5, -0.49, 0.48, 0.25, -2.2, 0.15, -1.88, 0.09, 1.2, -3.91, -0.17, 4.29, -4.98, -4.23, -2.51, 4.79, 0.46, -4.06, -4.03, -3.72, 2.67, -0.21, 0.13, -3.67, -3.76, -1.03, 4.27, 4.89, -2.71, 4.64, -4.33, 2.26, -3.72, 4.09, -1.44, 0.87, 3.94, -2.0, -2.31, 4.7, 3.64, -3.89, -1.89, 4.88, -2.51, 4.03, 2.78, 3.68, 3.1, -2.83, -4.66, 3.05, -0.46, 0.91, 0.7, 1.64, -0.82, 1.64, -4.56, -2.72, 0.72, 3.75, 2.73, 2.47, 0.36, 0.52, 1.95, -4.63, -1.19, 1.2, -0.06, 0.12, -4.15, -0.38, -1.31, -4.05, -3.65, -1.45, -2.41, -1.82, -0.36, 3.7, -0.87, 4.25, -3.51, -4.85, 2.61, -4.56, -4.89, 1.71, 1.4, -3.16, -2.83, 1.03, 3.88, 0.74, -4.09, -2.08, 1.46, 1.99, 2.91, -1.45, 0.87, 0.01, 3.66, 3.27, 4.12, 4.7, -0.73, 2.0, -4.72, 2.73, -4.9, -0.49, -3.41, 3.6, 2.45, 2.72, 0.41, -0.18, -3.68, 0.9, 1.35, -4.72, 1.92, 1.35, 2.8, -0.17, 1.91, -4.83, 1.57, -2.13, -0.04, -1.15, 0.74, 2.94, -0.19, -2.99, -3.57, 4.26, 0.57, -0.19, 4.63, -4.17, -3.04, -0.84, -3.66, -1.33, 2.35, -4.38, 0.29, -0.09, -2.25, 4.68, 1.63, 1.95, -0.46, -1.45, 4.57, 3.4, -0.63, 2.14, -2.92, 1.78, -0.17, -0.05, 0.12, -2.0, -3.9, -0.9, 4.67, 2.12, -0.45, 1.16, -3.89, 1.54, 3.37, 2.51, -4.01, 3.1, 3.24, 1.84, 2.12, -4.46, -4.61, 2.41, 2.9, 0.65, -3.22, 4.28, -0.59, 3.61, -3.08, -0.64, -3.71, 4.98, 3.5, -4.28, 4.25, -4.0, 0.74, 1.28, -4.58, -2.68, -1.03, 1.33, -0.17, 3.11, -4.58, -4.12, 4.84, -4.53, -3.52, 1.78, -1.69, -1.37, -0.74, 3.4, 1.76, 2.11, -3.6, -0.01, 0.73, -4.14, -4.94, 0.05, 1.92, -4.29, 3.54, -2.99, -0.1, -1.1, 4.96, 1.49, -4.47, 2.15, 0.39, -0.29, -0.22, -2.26, -1.78, -1.81, 0.84, -2.46, -3.06, 1.42, -0.47, -1.15, 3.32, -4.21, -4.99, -0.83, -3.82, -1.43, -4.65, -2.15, -0.57, -1.76, -2.56, -0.34, -3.08, -1.51, -2.48, 1.74, -4.78, -2.13, -1.52, 3.06, 1.74, 3.86, -2.06, -1.32, 2.08, 3.23, -1.32, -0.81, -4.94, 0.08, -2.68, -3.82, 4.71, 4.24, 2.81, 3.81, -2.6, 4.96, -2.48, 2.76, 2.65, -1.08, 3.86, -1.5, 4.51, -4.06, 4.75, 2.19, -0.09, 1.4, 0.13, -4.02, -2.5, 0.3, 4.95, -1.53, -1.36, -2.54, -0.77, 2.34, 3.67, 1.14, 2.19, -4.57, 4.98, -1.32, 3.22, 1.81, 4.12, 4.18, -2.58, -4.55, 3.17, -0.67, 4.73, -2.81, 2.88, -0.26, -3.18, -4.46, 4.06, -2.98, 0.84, 0.19, -3.72, -2.99, 1.48, 0.92, -2.63, -2.41, 0.73, 3.89, -0.38, -2.73, 0.02, -0.77, -3.56, -2.24, 4.23, 0.54, 4.89, -0.62, 4.04, -0.47, -1.6, -2.92, 3.27, 0.55, -3.71, 4.9, -4.36, -4.51, 1.26, -1.35, -3.12, -4.99, -3.06, -1.22, -4.23, -2.68, -3.36, 2.32, -2.06, -3.35, -0.36, 1.19, 0.53, -2.12, 0.35, 5.0, 0.36, -1.36, -1.25, -0.08, -3.12, -4.12, -1.03, -2.69, 2.81, -2.71, -3.22, -2.4, 3.32, -3.83, -0.49, -2.72, 0.49, -1.1, 4.94, -3.76, 3.69, 3.76, 2.83, -0.98, 1.41, 4.34, 0.26, 4.47, -3.33, 0.4, -1.54, -1.18, 2.27, -1.61, -3.19, -1.43, 3.85, -4.18, 1.13, -0.29, 3.3, -4.48, -1.45, 1.61, -4.65, -3.1, -4.12, -3.39, -4.45, 1.75, 3.1, 2.28, 1.73, 3.34, 3.82, 2.51, 4.88, -0.77, 2.07, -3.22, -4.06, 3.88, -4.74, -3.95, 0.79, -3.38, -4.94, 0.62, -0.1, 1.04, -1.32, 1.0, -1.19, 0.53, 1.77, -2.65, -0.56, -1.13, 2.07, -1.39, 1.11, 0.55, 4.93, 2.61, 0.3, -3.99, -0.27, 3.8, 2.23, -0.88, -1.47, 2.23, 3.43, -1.35, 0.89, 0.52, 3.04, -1.5, 4.82, -3.79, -0.78, 4.09, 1.92, 3.32, 3.2, 4.02, -2.76, -1.59, -0.21, 4.67, -0.49, -0.78, -3.2, 4.03, -2.88, -1.56, -0.77, -1.93, -2.52, -3.36, 2.05, -4.78, -0.46, -4.22, 0.44, -1.62, -0.17, 1.98, -1.24, 4.22, -0.77, -2.04, 0.36, 4.62, -3.64, 1.7, -4.11, 1.98, -4.17, 1.35, 4.3, -0.9, -2.48, 0.71, 0.84, 1.86, 1.35, 4.05, -1.36, 2.44, -1.14, 0.09, 1.58, 4.13, 3.24, -2.16, -3.28, -4.4, 3.88, -0.44, 3.8, 1.1, -1.54, 0.68, 3.06, -0.55, -4.42, -2.36, -0.9, 4.63, -0.34, -1.45, -4.49, -0.3, -3.25, -1.13, 2.73, -3.58, -1.31, -5.0, 2.11, 1.08, -1.22, 2.11, 0.86, -4.41, 2.86, -3.75, 3.42, 1.28, 1.12, -4.4, 2.68, -2.72, -0.13, -2.72, -4.71, -0.75, -1.15, -1.45, -0.06, 0.77, -3.91, 3.39, -2.63, -3.74, -4.52, 2.1, 1.01, 0.7, 1.78, -1.23, -2.15, -0.82, 1.22, -0.01, 2.7, 1.09, 0.67, 1.72, -1.53, -3.18, -0.53, -1.91, 1.72, -0.97, -2.75, 2.78, -0.41, -1.33, 1.22, -2.96, 1.54, 1.91, 1.1, -1.74, 3.63, -4.59, 3.07, 0.52, -1.47, -2.56, 3.29, -3.42, 1.41, 2.54, 3.84, 2.43, 2.86, -0.16, 4.05, -2.0, 0.65, -0.76, 4.95, 3.04, 0.06, -3.43, -1.34, 0.53, 0.95, 1.18, 2.73, 2.37, 4.05, 1.17, 1.55, 4.06, 0.36, 3.92, 4.76, 1.97, 1.53, -4.19, 4.9, -0.54, -1.51, 1.42, -0.36, -1.43, 0.96, 3.8, 3.15, 4.18, 1.75, 2.44, 1.17, 4.69, 1.54, 3.01, 1.0, 3.95, -3.65, -0.54, -4.71, 3.15, -3.66, 4.96, 3.42, -1.32, -1.95, 0.1, 3.2, 2.86, -0.19, -1.82, 2.21, -2.03, 2.84, 4.62, -1.69, 1.13, -0.42, -4.32, 1.81, -4.28, 0.41, -0.41, -2.46, 0.04, 4.94, 3.12, 3.68, 1.51, 2.37, 2.45, 4.59, 3.75, -3.04, 3.77, 1.75, -3.02, 3.63, 5.0, 2.92, 2.95, 4.22, 1.29, 4.99, 2.83, 3.62, 0.51, 0.36, -3.04, 3.04, 4.51, 4.56, -3.62, 3.61, 3.81, 2.16, -2.72, -0.34, -4.16, 3.01, -2.19, 1.72, -0.74, -2.42, -2.76, 1.27, 3.32, -4.79, -3.92, 3.04, 2.03, 2.8, 1.99, -4.13, 1.17, 1.2, 4.01, -4.82, -2.42, 3.34, -0.1, 2.92, -0.98, 4.0, 1.09, 0.97, -2.21, 3.96, 0.83, 2.94, 0.68, 4.58, -0.13, 0.37, -4.8, 3.03, 1.49, -1.53, -4.27, 4.85, -4.31, 2.54, 3.19, -3.38, -1.51, 4.02, -2.4, -4.14, -4.16, 4.85, 2.07, 0.5, 2.54, -3.62, -1.62, -3.77, -1.47, -0.26, -0.84, 2.38, -3.5, 4.01, -2.27, -4.13, -2.12, -0.22, 3.54, 3.33, 0.33, 4.4, 0.86, 3.56, -0.6, -0.84, -3.1, -0.71, 1.41, 0.63, 0.66, -2.45, -0.11, 2.82, 1.02, 4.04, 1.41, 1.48, -1.51, 3.61, -0.06, -2.04, 2.69, 1.45, -2.3, -3.59, -2.12, -4.73, -0.98]\n",
            "------------------------------------\n",
            "Tiempos de ejecucion: \n",
            "* Tiempo Total:  18.463 [ms]\n",
            "* Tiempo GPU:  1.035 [ms]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}